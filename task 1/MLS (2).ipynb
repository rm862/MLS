{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wEcn8bwJKCP"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile task.py\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "# Your Task 1.1 code here\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "\n",
        "def distance_cosine(X, Y):\n",
        "    # Normalize vectors for cosine distance calculation\n",
        "    X_norm = torch.nn.functional.normalize(X, dim=1)\n",
        "    Y_norm = torch.nn.functional.normalize(Y, dim=1)\n",
        "    # Calculate cosine similarity\n",
        "    similarity = torch.matmul(X_norm, Y_norm.T)\n",
        "    # Clamp values to avoid numerical issues\n",
        "    similarity = torch.clamp(similarity, min=-1.0, max=1.0)\n",
        "    # Convert similarity to distance (1 - similarity)\n",
        "    return 1.0 - similarity\n",
        "\n",
        "def distance_l2(X, Y):\n",
        "    # Euclidean distance calculation using matrix operations\n",
        "    # ||x-y||^2 = ||x||^2 + ||y||^2 - 2xÂ·y\n",
        "    X_squared = (X ** 2).sum(dim=1, keepdim=True)\n",
        "    Y_squared = (Y ** 2).sum(dim=1, keepdim=True).T\n",
        "    XY = torch.matmul(X, Y.T)\n",
        "    distances = X_squared + Y_squared - 2 * XY\n",
        "    # Clamp to prevent negative values due to numerical precision\n",
        "    distances = torch.clamp(distances, min=1e-12)\n",
        "    return torch.sqrt(distances)\n",
        "\n",
        "def distance_dot(X, Y):\n",
        "    # Negative dot product as distance (higher dot product = lower distance)\n",
        "    return -torch.matmul(X, Y.T)\n",
        "\n",
        "def distance_manhattan(X, Y):\n",
        "    # Optimization: Use batch processing for large datasets\n",
        "    batch_size = 1024\n",
        "    n_x = X.shape[0]\n",
        "    n_y = Y.shape[0]\n",
        "\n",
        "    # For large matrices, use batched computation to save memory\n",
        "    if n_x * n_y > 10000000 and max(n_x, n_y) > batch_size:\n",
        "        result = torch.zeros((n_x, n_y), device=X.device)\n",
        "        for i in range(0, n_x, batch_size):\n",
        "            end_i = min(i + batch_size, n_x)\n",
        "            # Process one batch at a time\n",
        "            result[i:end_i] = torch.cdist(X[i:end_i], Y, p=1)\n",
        "        return result\n",
        "    else:\n",
        "        # For smaller matrices, compute all at once\n",
        "        return torch.cdist(X, Y, p=1)\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "# Your Task 1.2 code here\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "\n",
        "def custom_topk(distances, k):\n",
        "    # Implementation of top-k algorithm for finding k smallest distances\n",
        "    n_queries = distances.shape[0]\n",
        "    top_indices = torch.zeros((n_queries, k), dtype=torch.int64, device=distances.device)\n",
        "\n",
        "    for i in range(n_queries):\n",
        "        dist_row = distances[i]\n",
        "        for j in range(k):\n",
        "            if j == 0:\n",
        "                # For first neighbor, simply find minimum\n",
        "                _, min_idx = torch.min(dist_row, dim=0)\n",
        "            else:\n",
        "                # For subsequent neighbors, mask previously found indices\n",
        "                masked = dist_row.clone()\n",
        "                masked[top_indices[i, :j]] = float('inf')\n",
        "                _, min_idx = torch.min(masked, dim=0)\n",
        "            top_indices[i, j] = min_idx\n",
        "\n",
        "    return top_indices\n",
        "\n",
        "def our_knn(N, D, A, X, K, metric=\"L2\"):\n",
        "    # Use GPU if available for acceleration\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Convert inputs to tensors if needed and move to device\n",
        "    A_is_tensor = torch.is_tensor(A)\n",
        "    X_is_tensor = torch.is_tensor(X)\n",
        "\n",
        "    # Optimization: For large datasets, use batching to avoid memory issues\n",
        "    large_dataset = N > 100000  # Consider datasets with >100k vectors as large\n",
        "\n",
        "    # Handle input formatting for X (single or multiple queries)\n",
        "    if X_is_tensor:\n",
        "        if X.dim() == 1:\n",
        "            X = X.unsqueeze(0)  # Convert to 2D tensor for single query\n",
        "        num_queries = X.shape[0]\n",
        "    else:\n",
        "        X = np.asarray(X, dtype=np.float32)\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(1, -1)  # Convert to 2D array for single query\n",
        "        num_queries = X.shape[0]\n",
        "\n",
        "    # Initialize result tensor on CPU to save GPU memory\n",
        "    result_indices = torch.zeros((num_queries, K), dtype=torch.int64, device='cpu')\n",
        "\n",
        "    # Configure batch sizes for processing\n",
        "    query_batch_size = min(64, num_queries)  # Process up to 64 queries at once\n",
        "    db_batch_size = min(10000, N) if large_dataset else N  # Use smaller batches for large datasets\n",
        "\n",
        "    # Process queries in batches for memory efficiency\n",
        "    for q_start in range(0, num_queries, query_batch_size):\n",
        "        q_end = min(q_start + query_batch_size, num_queries)\n",
        "\n",
        "        # Create query batch tensor on device\n",
        "        if X_is_tensor:\n",
        "            X_batch = X[q_start:q_end].to(device)\n",
        "        else:\n",
        "            X_batch = torch.tensor(X[q_start:q_end], dtype=torch.float32, device=device)\n",
        "\n",
        "        if large_dataset:\n",
        "            # Optimization: For large datasets, process database in batches\n",
        "            # Initialize top K tracking for each query in batch\n",
        "            topk_values = torch.full((q_end - q_start, K), float('inf'), device=device)\n",
        "            topk_indices = torch.zeros((q_end - q_start, K), dtype=torch.int64, device=device)\n",
        "\n",
        "            # Process database in batches\n",
        "            for db_start in range(0, N, db_batch_size):\n",
        "                db_end = min(db_start + db_batch_size, N)\n",
        "\n",
        "                # Create database batch tensor on device\n",
        "                if A_is_tensor:\n",
        "                    A_batch = A[db_start:db_end].to(device)\n",
        "                else:\n",
        "                    A_batch = torch.tensor(A[db_start:db_end], dtype=torch.float32, device=device)\n",
        "\n",
        "                # Calculate distances using the appropriate metric\n",
        "                if metric.lower() == \"l2\":\n",
        "                    batch_dist = distance_l2(X_batch, A_batch)\n",
        "                elif metric.lower() == \"cosine\":\n",
        "                    batch_dist = distance_cosine(X_batch, A_batch)\n",
        "                elif metric.lower() == \"dot\":\n",
        "                    batch_dist = distance_dot(X_batch, A_batch)\n",
        "                elif metric.lower() == \"manhattan\":\n",
        "                    batch_dist = distance_manhattan(X_batch, A_batch)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "                # Update top-K for each query by combining results from each batch\n",
        "                batch_size = q_end - q_start\n",
        "                for i in range(batch_size):\n",
        "                    # Combine current results with new batch results\n",
        "                    curr_values = topk_values[i]\n",
        "                    curr_indices = topk_indices[i]\n",
        "\n",
        "                    # Get current batch distances and indices\n",
        "                    new_values = batch_dist[i]\n",
        "                    new_indices = torch.arange(db_start, db_end, device=device)\n",
        "\n",
        "                    # Combine and get new top-K\n",
        "                    if db_start > 0:  # Not the first batch\n",
        "                        all_values = torch.cat([curr_values, new_values])\n",
        "                        all_indices = torch.cat([curr_indices, new_indices])\n",
        "\n",
        "                        # Get top-K (smallest for L2, cosine, manhattan; largest for dot)\n",
        "                        if metric.lower() == \"dot\":\n",
        "                            topk = torch.topk(all_values, min(K, len(all_values)), largest=True)\n",
        "                        else:\n",
        "                            topk = torch.topk(all_values, min(K, len(all_values)), largest=False)\n",
        "\n",
        "                        topk_values[i] = topk.values\n",
        "                        topk_indices[i] = all_indices[topk.indices]\n",
        "                    else:  # First batch, just take top-K\n",
        "                        if metric.lower() == \"dot\":\n",
        "                            topk = torch.topk(new_values, min(K, len(new_values)), largest=True)\n",
        "                        else:\n",
        "                            topk = torch.topk(new_values, min(K, len(new_values)), largest=False)\n",
        "\n",
        "                        topk_values[i, :len(topk.values)] = topk.values\n",
        "                        topk_indices[i, :len(topk.indices)] = new_indices[topk.indices]\n",
        "\n",
        "                # Memory optimization: clear batch data after processing\n",
        "                del A_batch, batch_dist\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            # Store results for this query batch\n",
        "            result_indices[q_start:q_end] = topk_indices.cpu()\n",
        "\n",
        "        else:\n",
        "            # For smaller datasets, process entire database at once (more efficient)\n",
        "            A_device = A.to(device) if A_is_tensor else torch.tensor(A, dtype=torch.float32, device=device)\n",
        "\n",
        "            # Calculate distances using the appropriate metric\n",
        "            if metric.lower() == \"l2\":\n",
        "                distances = distance_l2(X_batch, A_device)\n",
        "            elif metric.lower() == \"cosine\":\n",
        "                distances = distance_cosine(X_batch, A_device)\n",
        "            elif metric.lower() == \"dot\":\n",
        "                distances = distance_dot(X_batch, A_device)\n",
        "            elif metric.lower() == \"manhattan\":\n",
        "                distances = distance_manhattan(X_batch, A_device)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "            # Get top-K indices\n",
        "            if metric.lower() == \"dot\":\n",
        "                _, indices = torch.topk(distances, K, dim=1, largest=True)\n",
        "            else:\n",
        "                _, indices = torch.topk(distances, K, dim=1, largest=False)\n",
        "\n",
        "            # Store results\n",
        "            result_indices[q_start:q_end] = indices.cpu()\n",
        "\n",
        "            # Memory optimization: clear data when done with non-tensor input\n",
        "            if not A_is_tensor:\n",
        "                del A_device\n",
        "\n",
        "        # Memory optimization: clear batch data after processing\n",
        "        del X_batch\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    return result_indices\n",
        "\n",
        "def benchmark_knn(N, D, K, metric=\"L2\"):\n",
        "    # Generate random data for benchmarking\n",
        "    A = np.random.randn(N, D).astype(np.float32)\n",
        "    X = np.random.randn(100, D).astype(np.float32)\n",
        "\n",
        "    # Time the KNN query execution\n",
        "    start = time.time()\n",
        "    _ = our_knn(N, D, A, X, K, metric)\n",
        "    return time.time() - start\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "# Your Task 2.1 code here\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "\n",
        "def distance_kernel(X, Y, metric=\"L2\"):\n",
        "    # Optimized distance calculation between matrix X and Y\n",
        "    if metric == \"L2\":\n",
        "        X2 = (X ** 2).sum(dim=1, keepdim=True)\n",
        "        Y2 = (Y ** 2).sum(dim=1, keepdim=True).T\n",
        "        return torch.sqrt(torch.clamp(X2 + Y2 - 2 * X @ Y.T, min=0.0))\n",
        "    elif metric == \"cosine\":\n",
        "        # Normalize vectors and compute cosine distance\n",
        "        X_norm = X / (X.norm(dim=1, keepdim=True) + 1e-8)  # Add epsilon to avoid division by zero\n",
        "        Y_norm = Y / (Y.norm(dim=1, keepdim=True) + 1e-8)\n",
        "        return 1 - torch.mm(X_norm, Y_norm.T)\n",
        "    else:\n",
        "        raise ValueError(\"metric must be 'L2' or 'cosine'\")\n",
        "\n",
        "def our_kmeans(N, D, A, K, metric=\"L2\", max_iters=100, tol=1e-4, use_gpu=True):\n",
        "    # Select device based on availability and user preference\n",
        "    device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Running K-Means on PyTorch ({device.upper()} backend), metric={metric}\")\n",
        "\n",
        "    # Convert input to tensor and move to device\n",
        "    A_tensor = torch.tensor(A, dtype=torch.float32).contiguous()\n",
        "    if use_gpu and A_tensor.device.type == \"cpu\":\n",
        "        # Use pin_memory for faster host-to-device transfer\n",
        "        A_tensor = A_tensor.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        A_tensor = A_tensor.to(device)\n",
        "\n",
        "    # Initialize centroids randomly from data points\n",
        "    # centroids = A_tensor[torch.randperm(N)[:K]].clone()\n",
        "    centroids = A_tensor[torch.randperm(N, device=device)[:K]].clone()\n",
        "\n",
        "    # Main K-means loop\n",
        "    for i in range(max_iters):\n",
        "        # Calculate distances from each point to each centroid\n",
        "        distances = distance_kernel(A_tensor, centroids, metric=metric)\n",
        "\n",
        "        # Assign each point to nearest centroid\n",
        "        labels = torch.argmin(distances, dim=1)\n",
        "\n",
        "        # Update centroids as mean of assigned points\n",
        "        new_centroids = torch.zeros_like(centroids, device=device)\n",
        "        ones = torch.ones_like(labels, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Count points per cluster\n",
        "        counts = torch.zeros(K, dtype=torch.float32, device=device).scatter_add_(0, labels, ones)\n",
        "\n",
        "        # Sum points per cluster\n",
        "        new_centroids.scatter_add_(0, labels.unsqueeze(1).expand(-1, D), A_tensor)\n",
        "\n",
        "        # Divide by count to get mean (avoid division by zero with clamp_min)\n",
        "        new_centroids /= counts.unsqueeze(1).clamp_min(1)\n",
        "\n",
        "        # Check for convergence\n",
        "        if torch.norm(new_centroids - centroids, p=2, dim=1).max().item() < tol:\n",
        "            print(f\"K-Means converged at iteration {i+1}\")\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return labels.cpu()\n",
        "\n",
        "\n",
        "\n",
        "def our_kmeans_modified(N, D, A, num_clusters, metric=\"L2\", max_iters=100, tol=1e-4, use_gpu=True):\n",
        "    \"\"\"\n",
        "    KMeans clustering with GPU/CPU support.\n",
        "\n",
        "    Parameters:\n",
        "      N: Number of data points.\n",
        "      D: Data dimension.\n",
        "      A: Numpy array of shape (N, D).\n",
        "      num_clusters: Number of clusters to form.\n",
        "      metric: \"L2\" or \"cosine\" used to compute distances.\n",
        "      max_iters: Maximum iterations.\n",
        "      tol: Convergence threshold.\n",
        "      use_gpu: If True, use CUDA if available.\n",
        "\n",
        "    Returns:\n",
        "      labels: Tensor of shape (N,) with cluster assignments.\n",
        "      centroids: Tensor of shape (num_clusters, D) representing cluster centers.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Check if A is already a torch tensor and move to correct device\n",
        "    if torch.is_tensor(A):\n",
        "        A_tensor = A\n",
        "        if A_tensor.device.type != device:\n",
        "            A_tensor = A_tensor.to(device)\n",
        "    else:\n",
        "        A_tensor = torch.tensor(A, dtype=torch.float32).contiguous()\n",
        "        if use_gpu and torch.cuda.is_available():\n",
        "            A_tensor = A_tensor.to(device)\n",
        "\n",
        "    # Initialize centroids using k-means++ initialization for better clustering quality\n",
        "    centroids = torch.zeros((num_clusters, D), dtype=torch.float32, device=device)\n",
        "\n",
        "    # Choose first centroid randomly\n",
        "    # first_idx = torch.randint(0, N, (1,)).item()\n",
        "    first_idx = torch.randint(0, N, (1,), device=device).item()\n",
        "    centroids[0] = A_tensor[first_idx].clone()\n",
        "\n",
        "    # Choose remaining centroids using k-means++ algorithm (weighted sampling based on distance)\n",
        "    for i in range(1, num_clusters):\n",
        "        # Calculate distances to closest existing centroid\n",
        "        min_dists = torch.min(distance_kernel(A_tensor, centroids[:i], metric=metric), dim=1)[0]\n",
        "\n",
        "        # Square distances and normalize to create a probability distribution\n",
        "        # Points farther from existing centroids have higher probability\n",
        "        probs = min_dists ** 2\n",
        "        probs /= probs.sum()\n",
        "\n",
        "        # Sample next centroid based on probability distribution\n",
        "        next_idx = torch.multinomial(probs, 1).item()\n",
        "        centroids[i] = A_tensor[next_idx].clone()\n",
        "\n",
        "    # Main K-means loop\n",
        "    for i in range(max_iters):\n",
        "        distances = distance_kernel(A_tensor, centroids, metric=metric)  # (N, num_clusters)\n",
        "        labels = torch.argmin(distances, dim=1)  # (N,)\n",
        "\n",
        "        # Update centroids\n",
        "        new_centroids = torch.zeros_like(centroids, device=device)\n",
        "        ones = torch.ones_like(labels, dtype=torch.float32, device=device)\n",
        "        counts = torch.zeros(num_clusters, dtype=torch.float32, device=device)\n",
        "        counts = counts.scatter_add_(0, labels, ones)\n",
        "        new_centroids = new_centroids.scatter_add_(0, labels.unsqueeze(1).expand(-1, D), A_tensor)\n",
        "        counts = counts.unsqueeze(1).clamp_min(1)  # Prevent division by zero\n",
        "        new_centroids /= counts\n",
        "\n",
        "        # Check for convergence\n",
        "        shift = torch.norm(new_centroids - centroids, p=2, dim=1).max().item()\n",
        "        centroids = new_centroids\n",
        "        if shift < tol:\n",
        "            print(f\"K-Means converged at iteration {i + 1}\")\n",
        "            break\n",
        "\n",
        "    return labels.cpu(), centroids.cpu()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "# Your Task 2.2 code here\n",
        "# ------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Optimized Approximate Nearest Neighbors Implementation with Recall Guarantees\n",
        "\n",
        "def calculate_clusters_for_recall(K, desired_recall=0.7):\n",
        "    \"\"\"\n",
        "    Calculate how many clusters need to be searched to achieve the desired recall rate.\n",
        "    Uses the formula L â¥ (K-1)/(1-r) where:\n",
        "    - K is the number of nearest neighbors we want\n",
        "    - r is the desired recall rate (0.0 to 1.0)\n",
        "    - L is the number of clusters to search\n",
        "\n",
        "    Returns: Number of clusters to search (L)\n",
        "    \"\"\"\n",
        "    if desired_recall >= 1.0:\n",
        "        # If perfect recall is desired, we'd need to search all clusters\n",
        "        return float('inf')\n",
        "\n",
        "    # Mathematical formula to guarantee recall rate\n",
        "    L = (K - 1) / (1 - desired_recall)\n",
        "    return max(int(np.ceil(L)), 1)  # Ensure at least 1 cluster is searched\n",
        "\n",
        "def our_ann_improved(N, D, A, X, K, num_clusters=None, desired_recall=0.7, metric=\"L2\", max_iters=100, tol=1e-4, use_gpu=True):\n",
        "    \"\"\"\n",
        "    Optimized Approximate Nearest Neighbor (ANN) algorithm with mathematical recall guarantees.\n",
        "\n",
        "    Parameters:\n",
        "      N: Number of vectors.\n",
        "      D: Dimension of each vector.\n",
        "      A: Numpy array of shape (N, D).\n",
        "      X: Query vector (e.g. numpy array of shape (D,)).\n",
        "      K: Number of nearest neighbors to return.\n",
        "      num_clusters: Number of clusters for KMeans (auto-calculated if None).\n",
        "      desired_recall: Target recall rate (0.0 to 1.0) for the algorithm.\n",
        "      metric: \"L2\" or \"cosine\" distance metric.\n",
        "      max_iters, tol, use_gpu: Parameters for KMeans.\n",
        "\n",
        "    Returns:\n",
        "      A list of indices (of length K) indicating the top K nearest neighbors in A.\n",
        "    \"\"\"\n",
        "    # Set device first and be consistent\n",
        "    device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Auto-tune parameters based on dataset size and desired recall\n",
        "    if num_clusters is None:\n",
        "        # Calculate optimal number of clusters based on data size\n",
        "        # Optimization: use sqrt(N) as a heuristic but cap min/max values\n",
        "        num_clusters = min(max(int(np.sqrt(N)), 10), 200)\n",
        "\n",
        "    # Calculate K1 (number of clusters to search) based on recall guarantee formula\n",
        "    K1 = calculate_clusters_for_recall(K, desired_recall)\n",
        "    K1 = min(K1, num_clusters)  # Cannot search more clusters than exist\n",
        "\n",
        "    # Calculate K2 (candidates per cluster) based on dimensionality scaling\n",
        "    # Optimization: Higher dimensions need more candidates due to curse of dimensionality\n",
        "    dim_factor = max(1.0, np.log10(D) / 2)\n",
        "    K2 = int(K * max(5, dim_factor))\n",
        "    K2 = min(K2, 200)  # Cap to avoid excessive computation\n",
        "\n",
        "    print(f\"ANN parameters (recall guarantee): num_clusters={num_clusters}, K1={K1}, K2={K2}, desired_recall={desired_recall}\")\n",
        "\n",
        "    # Step 1: Convert inputs to tensors and move to the correct device\n",
        "    # Convert A to tensor if it's not already\n",
        "    if torch.is_tensor(A):\n",
        "        A_tensor = A.to(device)\n",
        "    else:\n",
        "        A_tensor = torch.tensor(A, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Convert X to tensor if it's not already\n",
        "    if torch.is_tensor(X):\n",
        "        X_tensor = X.to(device)\n",
        "    else:\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Ensure X has the right shape\n",
        "    if X_tensor.dim() == 1:\n",
        "        X_tensor = X_tensor.unsqueeze(0)  # shape (1, D)\n",
        "\n",
        "    # Step 2: Cluster A into num_clusters clusters using KMeans\n",
        "    labels, centroids = our_kmeans_modified(N, D, A_tensor, num_clusters,\n",
        "                                        metric=metric, max_iters=max_iters, tol=tol,\n",
        "                                        use_gpu=use_gpu)\n",
        "\n",
        "    # Convert labels and centroids to tensors on the correct device\n",
        "    if not torch.is_tensor(labels):\n",
        "        labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
        "    else:\n",
        "        labels = labels.to(device)\n",
        "\n",
        "    if not torch.is_tensor(centroids):\n",
        "        centroids = torch.tensor(centroids, dtype=torch.float32, device=device)\n",
        "    else:\n",
        "        centroids = centroids.to(device)\n",
        "\n",
        "    # Step 3: Calculate cluster statistics for density-aware search\n",
        "    cluster_sizes = torch.zeros(num_clusters, dtype=torch.float32, device=device)\n",
        "    for c in range(num_clusters):\n",
        "        cluster_sizes[c] = torch.sum(labels == c).float()\n",
        "\n",
        "    # Compute density-based weights for each cluster\n",
        "    # Optimization: Consider cluster density in priority calculation\n",
        "    total_points = torch.sum(cluster_sizes).float()\n",
        "    cluster_weights = cluster_sizes / total_points\n",
        "\n",
        "    # Step 4: Calculate distance from query to each centroid\n",
        "    if metric == \"L2\":\n",
        "        centroid_dist = torch.cdist(X_tensor, centroids, p=2).squeeze(0)\n",
        "    elif metric == \"cosine\":\n",
        "        X_norm = X_tensor / (torch.norm(X_tensor, dim=1, keepdim=True) + 1e-8)\n",
        "        centroids_norm = centroids / (torch.norm(centroids, dim=1, keepdim=True) + 1e-8)\n",
        "        centroid_dist = 1 - torch.matmul(X_norm, centroids_norm.t()).squeeze(0)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported metric: {metric}\")\n",
        "\n",
        "    # Step 5: Create a priority score for each cluster combining distance and density\n",
        "    # Optimization: Adaptive radius factor based on dimensionality\n",
        "    dim_radius_factor = 1.0 + (D / 1000)\n",
        "\n",
        "    # Softmax-normalize distances to get probability distribution\n",
        "    exp_neg_dist = torch.exp(-centroid_dist / dim_radius_factor)\n",
        "    softmax_probs = exp_neg_dist / torch.sum(exp_neg_dist)\n",
        "\n",
        "    # Combine distance probability with density weight\n",
        "    # Optimization: Balance between distance and density with alpha parameter\n",
        "    alpha = 0.7  # Weight for distance vs density\n",
        "    cluster_scores = alpha * softmax_probs + (1 - alpha) * cluster_weights\n",
        "\n",
        "    # Get top K1 clusters based on combined score\n",
        "    _, ranked_clusters = torch.topk(cluster_scores, min(K1, num_clusters))\n",
        "    selected_clusters = ranked_clusters.cpu().numpy().tolist()\n",
        "\n",
        "    # Step 6: Process selected clusters to find candidate neighbors\n",
        "    unique_candidate_indices = set()\n",
        "    candidate_with_dist = []\n",
        "\n",
        "    for c in selected_clusters:\n",
        "        # Get indices of all points in this cluster\n",
        "        cluster_indices = torch.nonzero(labels == c, as_tuple=True)[0]\n",
        "\n",
        "        if cluster_indices.numel() == 0:\n",
        "            continue\n",
        "\n",
        "        # Calculate distances between query and points in this cluster\n",
        "        cluster_points = A_tensor[cluster_indices]\n",
        "\n",
        "        if metric == \"L2\":\n",
        "            dists = torch.cdist(X_tensor, cluster_points, p=2).squeeze(0)\n",
        "        elif metric == \"cosine\":\n",
        "            X_norm = X_tensor / (torch.norm(X_tensor, dim=1, keepdim=True) + 1e-8)\n",
        "            cluster_norm = cluster_points / (torch.norm(cluster_points, dim=1, keepdim=True) + 1e-8)\n",
        "            dists = 1 - torch.matmul(X_norm, cluster_norm.t()).squeeze(0)\n",
        "\n",
        "        # Get top K2 candidates from this cluster\n",
        "        topk = min(K2, cluster_points.size(0))\n",
        "        _, cluster_topk = torch.topk(-dists, topk, largest=True)\n",
        "\n",
        "        # Add candidates with their distances to our list\n",
        "        for i in range(len(cluster_topk)):\n",
        "            idx = cluster_indices[cluster_topk[i]].item()\n",
        "            dist = dists[cluster_topk[i]].item()\n",
        "            if idx not in unique_candidate_indices:\n",
        "                unique_candidate_indices.add(idx)\n",
        "                candidate_with_dist.append((dist, idx))\n",
        "\n",
        "    # Step 7: Final candidate selection\n",
        "    if len(candidate_with_dist) < K:\n",
        "        # Not enough candidates, perform exact search\n",
        "        # Fallback strategy to guarantee results even with insufficient candidates\n",
        "        print(\"Warning: Not enough candidates found in selected clusters, performing exact search\")\n",
        "\n",
        "        if metric == \"L2\":\n",
        "            dists = torch.cdist(X_tensor, A_tensor, p=2).squeeze(0)\n",
        "        elif metric == \"cosine\":\n",
        "            X_norm = X_tensor / (torch.norm(X_tensor, dim=1, keepdim=True) + 1e-8)\n",
        "            A_norm = A_tensor / (torch.norm(A_tensor, dim=1, keepdim=True) + 1e-8)\n",
        "            dists = 1 - torch.matmul(X_norm, A_norm.t()).squeeze(0)\n",
        "\n",
        "        _, indices = torch.topk(-dists, min(K, N), largest=True)\n",
        "        return indices.cpu().numpy().tolist()\n",
        "\n",
        "    # Sort candidates by distance and return top K\n",
        "    candidate_with_dist.sort()  # Sort by distance (first element of tuple)\n",
        "    top_candidates = [idx for _, idx in candidate_with_dist[:K]]\n",
        "\n",
        "    return top_candidates\n",
        "\n",
        "# Main Benchmark Runner\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Speed benchmark for D=2:\")\n",
        "    print(f\"Time for 10,000 vectors with D=2: {benchmark_knn(10000, 2, 10):.4f} seconds\")\n",
        "    print(\"\\nSpeed benchmark for D=2^15:\")\n",
        "    print(f\"Time for 1,000 vectors with D=2^15: {benchmark_knn(1000, 2**15, 10):.4f} seconds\")\n",
        "    print(\"\\nProcessing 4,000 vectors:\")\n",
        "    print(f\"Time for 4,000 vectors: {benchmark_knn(4000, 128, 10):.4f} seconds\")\n",
        "    print(\"\\nProcessing simulation for 4,000,000 vectors:\")\n",
        "    sample_time = benchmark_knn(40000, 128, 10)\n",
        "    print(f\"Estimated time for 4,000,000 vectors: {sample_time * (4000000 / 40000):.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHkuPXY_wiFC",
        "outputId": "2e77b35c-7b91-4942-e150-8e95c9968eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting task.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test.py\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from task import our_knn, our_kmeans, our_ann_improved\n",
        "\n",
        "def generate_data(size, for_knn=False):\n",
        "    \"\"\"\n",
        "    Generate synthetic data for testing KNN or K-means algorithms\n",
        "    Parameters:\n",
        "        size: String indicating dataset size (\"small\", \"medium\", \"large\", etc.)\n",
        "        for_knn: Boolean indicating if data is for KNN (includes query vectors)\n",
        "    Returns:\n",
        "        N, D, A, [X], K: Dataset parameters, data matrix, [query vectors], number of neighbors/clusters\n",
        "    \"\"\"\n",
        "    if size == \"small\":\n",
        "        N, D, K = 100, 2, 5      # Small dataset: 100 vectors, 2 dimensions, 5 neighbors/clusters\n",
        "        M = 10                   # 10 query vectors\n",
        "    elif size == \"medium\":\n",
        "        N, D, K = 4000, 128, 10  # Medium: 4,000 vectors, 128 dimensions\n",
        "        M = 100\n",
        "    elif size == \"large\":\n",
        "        N, D, K = 10000, 1024, 20  # Large: 10,000 vectors, 1024 dimensions\n",
        "        M = 100\n",
        "    elif size == \"large_dim2\":\n",
        "        N, D, K = 10000, 2, 10   # Testing with very low dimension but large number of vectors\n",
        "        M = 100\n",
        "    elif size == \"large_dim32k\":\n",
        "        N, D, K = 1000, 2**15, 10  # Testing with very high dimension (32,768)\n",
        "        M = 10\n",
        "    elif size == \"huge\":\n",
        "        # Simulate large dataset performance without using full 4M vectors\n",
        "        N, D, K = 40000, 128, 10  # Use 40,000 as representative for 4,000,000\n",
        "        M = 100\n",
        "        print(\"Note: Using 40,000 vectors to extrapolate performance for 4,000,000 vectors\")\n",
        "\n",
        "    # Generate random data with normal distribution (float32 for GPU compatibility)\n",
        "    A = np.random.randn(N, D).astype(np.float32)\n",
        "\n",
        "    if for_knn:\n",
        "        # For KNN we need query vectors\n",
        "        X = np.random.randn(M, D).astype(np.float32)\n",
        "        return N, D, A, X, K\n",
        "    else:\n",
        "        # For K-means we only need the data matrix\n",
        "        return N, D, A, K\n",
        "\n",
        "\n",
        "def run_knn(N, D, A, X, K, metric, use_gpu, label):\n",
        "    \"\"\"\n",
        "    Run and benchmark the KNN implementation\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running KNN with {metric.upper()} on {label.upper()} ({N} db, {X.shape[0]} queries, {D} dimensions) ---\")\n",
        "\n",
        "    # Ensure GPU environment is properly set before running KNN\n",
        "    if use_gpu:\n",
        "        if torch.cuda.is_available():\n",
        "            # Synchronize and clear cache for consistent benchmarking\n",
        "            torch.cuda.synchronize()\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            print(\"Warning: GPU requested but not available. Falling back to CPU.\")\n",
        "\n",
        "    # Measure execution time\n",
        "    start = time.time()\n",
        "    indices = our_knn(N, D, A, X, K, metric=metric)\n",
        "\n",
        "    # Ensure all GPU operations complete before stopping timer\n",
        "    if use_gpu and torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    end = time.time()\n",
        "    print(f\"First 3 query results:\\n{indices[:3]}\")\n",
        "    print(f\"Time: {end - start:.4f} sec\")\n",
        "\n",
        "def compare_cpu_gpu(size, metric=\"L2\"):\n",
        "    \"\"\"\n",
        "    Directly compare CPU and GPU performance on the same dataset\n",
        "    \"\"\"\n",
        "    print(f\"\\n======= Comparing CPU vs GPU on {size.upper()} dataset with {metric} =======\")\n",
        "\n",
        "    # Generate data for KNN test\n",
        "    N, D, A, X, K = generate_data(size, for_knn=True)\n",
        "\n",
        "    # Run on CPU first\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    cpu_start = time.time()\n",
        "    our_knn(N, D, A, X, K, metric=metric)\n",
        "    cpu_end = time.time()\n",
        "    cpu_time = cpu_end - cpu_start\n",
        "\n",
        "    # Run on GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        # Set default tensor type to CUDA for GPU operations\n",
        "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "        torch.cuda.synchronize()  # Ensure GPU is synchronized before timing\n",
        "        gpu_start = time.time()\n",
        "        our_knn(N, D, A, X, K, metric=metric)\n",
        "        torch.cuda.synchronize()  # Ensure all GPU operations complete\n",
        "        gpu_end = time.time()\n",
        "        gpu_time = gpu_end - gpu_start\n",
        "\n",
        "        # Calculate and display speedup\n",
        "        speedup = cpu_time / gpu_time\n",
        "        print(f\"CPU time: {cpu_time:.4f} sec\")\n",
        "        print(f\"GPU time: {gpu_time:.4f} sec\")\n",
        "        print(f\"GPU speedup: {speedup:.2f}x\")\n",
        "    else:\n",
        "        print(\"GPU not available, skipping comparison\")\n",
        "\n",
        "def extrapolate_large_dataset():\n",
        "    \"\"\"\n",
        "    Estimate performance for very large datasets by extrapolating from smaller samples\n",
        "    \"\"\"\n",
        "    print(\"\\n======= Extrapolating performance for 4,000,000 vectors =======\")\n",
        "\n",
        "    # Test with 40,000 vectors to estimate performance for 4M\n",
        "    N, D, A, X, K = generate_data(\"huge\", for_knn=True)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        # Use GPU for extrapolation\n",
        "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        our_knn(N, D, A, X, K, metric=\"L2\")\n",
        "        torch.cuda.synchronize()\n",
        "        end = time.time()\n",
        "        measured_time = end - start\n",
        "\n",
        "        # Linear extrapolation - assuming O(n) scaling which is optimistic\n",
        "        # Actual scaling may be worse depending on algorithm implementation\n",
        "        estimated_time = measured_time * (4000000 / N)\n",
        "        print(f\"Time for {N} vectors: {measured_time:.4f} sec\")\n",
        "        print(f\"Estimated time for 4,000,000 vectors: {estimated_time:.2f} sec\")\n",
        "        print(f\"Optimizations needed for 4M vectors: batching, memory management, results streaming\")\n",
        "    else:\n",
        "        print(\"GPU not available, skipping extrapolation\")\n",
        "\n",
        "def compare_all():\n",
        "    import traceback\n",
        "\n",
        "    print(\"\\n======= [KNN] Comparing CPU vs GPU =======\")\n",
        "    knn_metrics = [\"L2\", \"cosine\", \"dot\", \"manhattan\"]\n",
        "    knn_datasets = [\"small\", \"medium\", \"large\", \"large_dim2\", \"large_dim32k\", \"huge\"]\n",
        "    for size in knn_datasets:\n",
        "        for metric in knn_metrics:\n",
        "            try:\n",
        "                print(f\"\\n--- [KNN] Comparing on {size.upper()} with {metric.upper()} ---\")\n",
        "                compare_cpu_gpu(size, metric)\n",
        "            except Exception as e:\n",
        "                print(f\"[KNN COMPARISON] Skipped {size.upper()} with {metric.upper()} due to error: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "    print(\"\\n======= [KMeans] Comparing CPU vs GPU =======\")\n",
        "    kmeans_metrics = [\"L2\", \"cosine\"]\n",
        "    kmeans_datasets = [\"small\", \"large\"]\n",
        "    for size in kmeans_datasets:\n",
        "        for metric in kmeans_metrics:\n",
        "            try:\n",
        "                print(f\"\\n--- [KMEANS] Running on {size.upper()} with {metric.upper()} ---\")\n",
        "                N, D, A, K = generate_data(size)\n",
        "\n",
        "                # CPU run\n",
        "                cpu_start = time.time()\n",
        "                our_kmeans(N, D, A, K, metric=metric, use_gpu=False)\n",
        "                cpu_time = time.time() - cpu_start\n",
        "\n",
        "                # GPU run\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.synchronize()\n",
        "                    gpu_start = time.time()\n",
        "                    our_kmeans(N, D, A, K, metric=metric, use_gpu=True)\n",
        "                    torch.cuda.synchronize()\n",
        "                    gpu_time = time.time() - gpu_start\n",
        "                    print(f\"CPU time: {cpu_time:.4f} sec\")\n",
        "                    print(f\"GPU time: {gpu_time:.4f} sec\")\n",
        "                    print(f\"GPU speedup: {cpu_time / gpu_time:.2f}x\")\n",
        "                else:\n",
        "                    print(\"GPU not available, skipped GPU comparison\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[KMEANS COMPARISON] Skipped {size.upper()} with {metric.upper()} due to error: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "    print(\"\\n======= [ANN] Comparing CPU vs GPU =======\")\n",
        "    ann_metrics = [\"L2\", \"cosine\"]\n",
        "    ann_datasets = [\"small\", \"large\"]\n",
        "    for size in ann_datasets:\n",
        "        for metric in ann_metrics:\n",
        "            try:\n",
        "                print(f\"\\n--- [ANN] Running on {size.upper()} with {metric.upper()} ---\")\n",
        "                N, D, A, X, K = generate_data(size, for_knn=True)\n",
        "\n",
        "                # CPU run\n",
        "                cpu_start = time.time()\n",
        "                our_ann_improved(N, D, A, X[0], K, metric=metric, use_gpu=False)\n",
        "                cpu_time = time.time() - cpu_start\n",
        "\n",
        "                # GPU run\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.synchronize()\n",
        "                    gpu_start = time.time()\n",
        "                    our_ann_improved(N, D, A, X[0], K, metric=metric, use_gpu=True)\n",
        "                    torch.cuda.synchronize()\n",
        "                    gpu_time = time.time() - gpu_start\n",
        "                    print(f\"CPU time: {cpu_time:.4f} sec\")\n",
        "                    print(f\"GPU time: {gpu_time:.4f} sec\")\n",
        "                    print(f\"GPU speedup: {cpu_time / gpu_time:.2f}x\")\n",
        "                else:\n",
        "                    print(\"GPU not available, skipped GPU comparison\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[ANN COMPARISON] Skipped {size.upper()} with {metric.upper()} due to error: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "    print(\"\\n======= Extrapolating for HUGE dataset =======\")\n",
        "    extrapolate_large_dataset()\n",
        "\n",
        "\n",
        "def run_kmeans(N, D, A, K, metric, use_gpu, label):\n",
        "    \"\"\"\n",
        "    Run and benchmark the K-means implementation\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Running metric: {metric} on {label.upper()} ({N} samples) ---\")\n",
        "    start = time.time()\n",
        "    labels = our_kmeans(N, D, A, K, metric=metric, use_gpu=use_gpu)\n",
        "    end = time.time()\n",
        "    print(f\"First 10 labels: {labels[:10].tolist()}\")\n",
        "    print(f\"Time on {label.upper()} ({metric}): {end - start:.4f} sec\")\n",
        "\n",
        "def run_ann_with_guarantees(N, D, A, X, K, metric, use_gpu, label):\n",
        "    \"\"\"\n",
        "    Run and benchmark the improved ANN implementation with recall guarantees\n",
        "\n",
        "    This is an optimization over traditional ANN by providing mathematical\n",
        "    guarantees on recall rates, ensuring a minimum quality of results.\n",
        "    \"\"\"\n",
        "    from task import our_ann_improved, our_knn\n",
        "    import numpy as np\n",
        "    import time\n",
        "    import torch\n",
        "\n",
        "    print(f\"\\n--- Running ANN (with recall guarantees) using {metric.upper()} on {label.upper()} ---\")\n",
        "    print(f\"Dataset: {N} vectors, {D} dimensions, {X.shape[0]} queries, K={K}\")\n",
        "\n",
        "    # Set the appropriate device (CPU/GPU)\n",
        "    device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Optimization: Convert data to tensors and move to device once\n",
        "    # This avoids repeated data transfers during processing\n",
        "    if torch.is_tensor(A):\n",
        "        A_tensor = A.to(device)\n",
        "    else:\n",
        "        A_tensor = torch.tensor(A, dtype=torch.float32, device=device)\n",
        "\n",
        "    # Test different target recall rates (percentage of true nearest neighbors found)\n",
        "    recall_targets = [0.7]  # Can be expanded to test multiple target recalls\n",
        "    num_test_queries = min(10, X.shape[0])\n",
        "\n",
        "    for target_recall in recall_targets:\n",
        "        print(f\"\\n=== Testing with target recall: {target_recall:.2f} ===\")\n",
        "\n",
        "        total_actual_recall = 0.0\n",
        "        total_ann_time = 0.0\n",
        "        total_knn_time = 0.0\n",
        "\n",
        "        # Optimization: Determine optimal number of clusters based on dataset size\n",
        "        num_clusters = min(max(int(np.sqrt(N)), 10), 200)\n",
        "\n",
        "        # Test multiple queries to get reliable statistics\n",
        "        for i in range(num_test_queries):\n",
        "            # Extract single query and ensure it's properly formatted\n",
        "            if X.ndim > 1:\n",
        "                query = X[i].reshape(1, -1)\n",
        "            else:\n",
        "                query = X.reshape(1, -1)\n",
        "\n",
        "            # Optimization: Move query to the correct device\n",
        "            if torch.is_tensor(query):\n",
        "                query_tensor = query.to(device)\n",
        "            else:\n",
        "                query_tensor = torch.tensor(query, dtype=torch.float32, device=device)\n",
        "\n",
        "            # Run improved ANN with recall guarantee\n",
        "            start = time.time()\n",
        "            ann_indices = our_ann_improved(N, D, A_tensor, query_tensor[0], K,\n",
        "                                         num_clusters=num_clusters,\n",
        "                                         desired_recall=target_recall,\n",
        "                                         metric=metric, use_gpu=use_gpu)\n",
        "\n",
        "            # Ensure timing captures all GPU operations\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            ann_time = time.time() - start\n",
        "            total_ann_time += ann_time\n",
        "\n",
        "            # Run exact KNN for comparison (ground truth)\n",
        "            start = time.time()\n",
        "            knn_indices = our_knn(N, D, A_tensor, query_tensor, K, metric=metric)\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "            knn_time = time.time() - start\n",
        "            total_knn_time += knn_time\n",
        "\n",
        "            # Calculate actual recall by comparing ANN results with exact KNN results\n",
        "            if torch.is_tensor(knn_indices):\n",
        "                knn_set = set(knn_indices[0].cpu().tolist())\n",
        "            else:\n",
        "                knn_set = set(knn_indices[0].tolist())\n",
        "\n",
        "            ann_set = set(ann_indices)\n",
        "            correct = len(ann_set.intersection(knn_set))\n",
        "            actual_recall = correct / K if K > 0 else 0\n",
        "            total_actual_recall += actual_recall\n",
        "\n",
        "            print(f\"Query {i+1}: Recall={actual_recall:.2f} ({correct}/{K}), \"\n",
        "                  f\"ANN: {ann_time:.4f}s, KNN: {knn_time:.4f}s\")\n",
        "\n",
        "        # Calculate and report average statistics\n",
        "        avg_recall = total_actual_recall / num_test_queries\n",
        "        avg_ann_time = total_ann_time / num_test_queries\n",
        "        avg_knn_time = total_knn_time / num_test_queries\n",
        "        avg_speedup = avg_knn_time / avg_ann_time if avg_ann_time > 0 else 0\n",
        "\n",
        "        print(f\"\\nTarget recall: {target_recall:.2f}\")\n",
        "        print(f\"Actual average recall: {avg_recall:.2f}\")\n",
        "        print(f\"Average times - ANN: {avg_ann_time:.4f}s, KNN: {avg_knn_time:.4f}s\")\n",
        "        print(f\"Average speedup: {avg_speedup:.2f}x\")\n",
        "\n",
        "        # Verify if recall guarantee was met\n",
        "        if avg_recall >= target_recall:\n",
        "            print(f\"â SUCCESS: Average recall ({avg_recall:.2f}) >= Target recall ({target_recall:.2f})\")\n",
        "        else:\n",
        "            print(f\"â FAILED: Average recall ({avg_recall:.2f}) < Target recall ({target_recall:.2f})\")\n",
        "\n",
        "def test_ann_with_guarantees(use_gpu):\n",
        "    \"\"\"\n",
        "    Run comprehensive tests for the improved ANN implementation across different datasets\n",
        "    \"\"\"\n",
        "    import torch\n",
        "\n",
        "    print(\"\\n======= Testing ANN with Mathematical Recall Guarantees =======\")\n",
        "\n",
        "    label = \"gpu\" if use_gpu else \"cpu\"\n",
        "\n",
        "    # Set up device environment\n",
        "    device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cuda\":\n",
        "        torch.cuda.empty_cache()  # Clear GPU memory before tests\n",
        "\n",
        "    # Test on multiple dataset sizes\n",
        "    for size in [\"small\", \"large\"]:\n",
        "        print(f\"\\n--- ANN Test on {size.upper()} dataset ---\")\n",
        "        N, D, A, X, K = generate_data(size, for_knn=True)\n",
        "\n",
        "        # Optimization: Pre-convert data to tensors and move to device once\n",
        "        # This avoids repeated data transfers during benchmarking\n",
        "        A_tensor = torch.tensor(A, dtype=torch.float32, device=device)\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "\n",
        "        # Test different distance metrics\n",
        "        for metric in [\"L2\", \"cosine\"]:\n",
        "            try:\n",
        "                run_ann_with_guarantees(N, D, A_tensor, X_tensor, K, metric, use_gpu, label)\n",
        "            except Exception as e:\n",
        "                print(f\"[ANN] Error with {metric.upper()} on {size} dataset: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run tests based on command line argument\n",
        "    \"\"\"\n",
        "    if len(sys.argv) != 2 or sys.argv[1] not in [\"gpu\", \"cpu\", \"comparison\", \"all\"]:\n",
        "        print(\"Usage: python test.py [gpu|cpu|comparison|all]\")\n",
        "        return\n",
        "\n",
        "    mode = sys.argv[1]\n",
        "\n",
        "    if mode == \"gpu\" or mode == \"all\":\n",
        "        # Run tests on GPU\n",
        "        use_gpu = True\n",
        "        label = \"gpu\"\n",
        "\n",
        "        # Set up GPU environment if available\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device(\"cuda\")\n",
        "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "            torch.set_default_tensor_type('torch.cuda.FloatTensor')  # Default to CUDA tensors\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "            print(\"Warning: GPU requested but not available. Using CPU.\")\n",
        "            torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "        metrics = [\"L2\", \"cosine\", \"dot\", \"manhattan\"]\n",
        "\n",
        "        # Test KNN on different dataset sizes\n",
        "        for size in [\"medium\", \"large_dim2\", \"large_dim32k\"]:\n",
        "            print(f\"\\n======= Testing on {size.upper()} dataset =======\")\n",
        "\n",
        "            # Generate data for KNN\n",
        "            N, D, A, X, K = generate_data(size, for_knn=True)\n",
        "            for metric in metrics:\n",
        "                try:\n",
        "                    run_knn(N, D, A, X, K, metric, use_gpu, label)\n",
        "                except Exception as e:\n",
        "                    print(f\"[KNN] Skipped {metric.upper()} due to error: {e}\")\n",
        "\n",
        "        # Test K-Means on GPU\n",
        "        for size in [\"small\", \"large\"]:\n",
        "            print(f\"\\n======= [KMeans] Testing on {size.upper()} dataset =======\")\n",
        "            N, D, A, K = generate_data(size)\n",
        "            for metric in [\"L2\", \"cosine\"]:\n",
        "                run_kmeans(N, D, A, K, metric, use_gpu, label)\n",
        "\n",
        "        # Test ANN with recall guarantees on GPU\n",
        "        test_ann_with_guarantees(use_gpu=True)\n",
        "\n",
        "    if mode == \"cpu\" or mode == \"all\":\n",
        "        # Run tests on CPU\n",
        "        use_gpu = False\n",
        "        label = \"cpu\"\n",
        "\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"Using CPU\")\n",
        "        torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "        metrics = [\"L2\", \"cosine\", \"dot\", \"manhattan\"]\n",
        "\n",
        "        # Test KNN on different dataset sizes\n",
        "        for size in [\"medium\", \"large_dim2\", \"large_dim32k\"]:\n",
        "            print(f\"\\n======= Testing on {size.upper()} dataset =======\")\n",
        "\n",
        "            # Generate data for KNN\n",
        "            N, D, A, X, K = generate_data(size, for_knn=True)\n",
        "            for metric in metrics:\n",
        "                try:\n",
        "                    run_knn(N, D, A, X, K, metric, use_gpu, label)\n",
        "                except Exception as e:\n",
        "                    print(f\"[KNN] Skipped {metric.upper()} due to error: {e}\")\n",
        "\n",
        "        # Test K-Means on CPU\n",
        "        for size in [\"small\", \"large\"]:\n",
        "            print(f\"\\n======= [KMeans] Testing on {size.upper()} dataset =======\")\n",
        "            N, D, A, K = generate_data(size)\n",
        "            for metric in [\"L2\", \"cosine\"]:\n",
        "                run_kmeans(N, D, A, K, metric, use_gpu, label)\n",
        "\n",
        "        # Test ANN with recall guarantees on CPU\n",
        "        test_ann_with_guarantees(use_gpu=False)\n",
        "\n",
        "    if mode == \"comparison\" or mode == \"all\":\n",
        "      compare_all()\n",
        "      extrapolate_large_dataset()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rrjl4QgEYoKC",
        "outputId": "e3841dc9-7869-4c1a-96ce-baa8401ae0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpnWnaZGYu9O",
        "outputId": "bae0f4ff-132c-44db-e0d1-13e09d869732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n",
            "/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "\n",
            "======= Testing on MEDIUM dataset =======\n",
            "\n",
            "--- Running KNN with L2 on GPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[  64, 3245, 1971, 2517, 3169,  640, 2389,  884,  969,  813],\n",
            "        [1312, 1650, 2245, 2083, 2501, 1338, 3287,  679, 2753,   62],\n",
            "        [2159, 2777, 1731, 2541, 2974, 3271, 1274,  474, 1098,  601]],\n",
            "       device='cpu')\n",
            "Time: 0.1571 sec\n",
            "\n",
            "--- Running KNN with COSINE on GPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[  64, 1971, 3245,  884, 1255, 3834,  783, 2389, 1299, 1642],\n",
            "        [1650, 3287,  282, 3732, 1312, 1091, 2821, 2738, 1540, 3643],\n",
            "        [2777, 2159, 1098, 3271, 3691, 2541, 3722, 3924, 1291,  482]],\n",
            "       device='cpu')\n",
            "Time: 0.0349 sec\n",
            "\n",
            "--- Running KNN with DOT on GPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[1507, 1209, 3173, 1683,  878, 1702, 2865, 3035, 3614, 2596],\n",
            "        [3879, 3469,   50, 2535, 3980, 2582, 3780, 3385,  408, 1029],\n",
            "        [2557, 3276, 3709, 2451,  356, 2872,    6, 2257, 2171,    3]],\n",
            "       device='cpu')\n",
            "Time: 0.0131 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on GPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[2517,  969, 2389, 3245, 1942, 1971,  640,  884,  921,  636],\n",
            "        [1312, 2083, 1338,   64, 3807, 2245, 1046,  265,  679, 2233],\n",
            "        [2159, 2777, 3924,  788, 1731, 1098,   85, 3294, 3271, 2541]],\n",
            "       device='cpu')\n",
            "Time: 0.0106 sec\n",
            "\n",
            "======= Testing on LARGE_DIM2 dataset =======\n",
            "\n",
            "--- Running KNN with L2 on GPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[9198, 3802, 8535, 9993, 9849, 2388, 7849, 9965, 1749, 2485],\n",
            "        [ 677, 4982, 9771, 8353, 7935, 9592, 9900,  298, 7812, 4225],\n",
            "        [1668, 6845,   34, 9702, 7090, 7396, 5917, 7475, 8643, 6661]],\n",
            "       device='cpu')\n",
            "Time: 0.0017 sec\n",
            "\n",
            "--- Running KNN with COSINE on GPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[2267, 8945, 4964, 9523, 1015, 4837, 3420, 6099, 8183, 2500],\n",
            "        [6379,  677, 3150, 7707,   86,  623, 3840, 8199, 4815, 5863],\n",
            "        [ 572, 6356, 6645, 1977, 2987, 7038, 2209, 5377, 5596, 7396]],\n",
            "       device='cpu')\n",
            "Time: 0.0015 sec\n",
            "\n",
            "--- Running KNN with DOT on GPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[2111, 9719, 9688, 2682, 9648, 6959, 6962, 3385, 6640,   60],\n",
            "        [ 512, 7202,  621, 6171, 5817, 2111, 9614, 9818, 6640, 6959],\n",
            "        [8929, 8704, 8112, 9257, 5164, 5003, 7565, 8406, 7184, 4708]],\n",
            "       device='cpu')\n",
            "Time: 0.0011 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on GPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[9198, 3802, 8535, 9993, 9849, 2485, 2388, 7849, 9965, 5059],\n",
            "        [ 677, 4982, 8353, 9771, 7935, 9900, 9592, 7812, 4225,  298],\n",
            "        [1668, 6845,   34, 9702, 7090, 8643, 5917, 7396, 7475, 2128]],\n",
            "       device='cpu')\n",
            "Time: 0.0182 sec\n",
            "\n",
            "======= Testing on LARGE_DIM32K dataset =======\n",
            "\n",
            "--- Running KNN with L2 on GPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[326, 537, 255, 756, 302, 442, 453, 775, 663,  78],\n",
            "        [297, 773, 975,  65, 396, 572,  61, 403, 183, 326],\n",
            "        [371,  36, 206, 756, 218, 709, 521, 926, 389, 925]], device='cpu')\n",
            "Time: 0.0335 sec\n",
            "\n",
            "--- Running KNN with COSINE on GPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[477, 946, 756, 326, 537, 255, 302, 274, 488, 653],\n",
            "        [297, 321,  61, 773, 283,  26, 572, 633,  65, 608],\n",
            "        [926, 371, 252, 947,  36, 800, 521, 206, 756, 516]], device='cpu')\n",
            "Time: 0.0398 sec\n",
            "\n",
            "--- Running KNN with DOT on GPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[422, 852, 228, 533, 439, 312, 784, 976, 658, 397],\n",
            "        [567,  52, 867, 859, 152, 400, 594, 595, 632, 223],\n",
            "        [733,   8, 498, 488, 916, 722, 489, 778, 423, 981]], device='cpu')\n",
            "Time: 0.0363 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on GPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[537, 326, 302, 255, 756, 453, 950, 663, 521, 325],\n",
            "        [ 65, 277, 297, 510,  61, 136, 313, 809, 265, 907],\n",
            "        [ 36, 413, 109,  77, 521, 469, 770, 371, 756, 206]], device='cpu')\n",
            "Time: 0.0379 sec\n",
            "\n",
            "======= [KMeans] Testing on SMALL dataset =======\n",
            "\n",
            "--- Running metric: L2 on GPU (100 samples) ---\n",
            "Running K-Means on PyTorch (CUDA backend), metric=L2\n",
            "K-Means converged at iteration 5\n",
            "First 10 labels: [2, 4, 0, 2, 3, 2, 3, 3, 1, 3]\n",
            "Time on GPU (L2): 0.0774 sec\n",
            "\n",
            "--- Running metric: cosine on GPU (100 samples) ---\n",
            "Running K-Means on PyTorch (CUDA backend), metric=cosine\n",
            "K-Means converged at iteration 6\n",
            "First 10 labels: [3, 2, 0, 3, 1, 3, 1, 1, 4, 1]\n",
            "Time on GPU (cosine): 0.0028 sec\n",
            "\n",
            "======= [KMeans] Testing on LARGE dataset =======\n",
            "\n",
            "--- Running metric: L2 on GPU (10000 samples) ---\n",
            "Running K-Means on PyTorch (CUDA backend), metric=L2\n",
            "K-Means converged at iteration 27\n",
            "First 10 labels: [3, 6, 13, 7, 4, 16, 17, 9, 19, 2]\n",
            "Time on GPU (L2): 0.0569 sec\n",
            "\n",
            "--- Running metric: cosine on GPU (10000 samples) ---\n",
            "Running K-Means on PyTorch (CUDA backend), metric=cosine\n",
            "K-Means converged at iteration 36\n",
            "First 10 labels: [4, 5, 10, 4, 9, 0, 0, 6, 10, 15]\n",
            "Time on GPU (cosine): 0.0584 sec\n",
            "\n",
            "======= Testing ANN with Mathematical Recall Guarantees =======\n",
            "\n",
            "--- ANN Test on SMALL dataset ---\n",
            "\n",
            "--- Running ANN (with recall guarantees) using L2 on GPU ---\n",
            "Dataset: 100 vectors, 2 dimensions, 10 queries, K=5\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 1: Recall=1.00 (5/5), ANN: 0.0822s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 2: Recall=1.00 (5/5), ANN: 0.0182s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 3: Recall=1.00 (5/5), ANN: 0.0180s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 4: Recall=1.00 (5/5), ANN: 0.0174s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 5: Recall=1.00 (5/5), ANN: 0.0165s, KNN: 0.0003s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 6: Recall=1.00 (5/5), ANN: 0.0151s, KNN: 0.0003s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 7: Recall=1.00 (5/5), ANN: 0.0156s, KNN: 0.0003s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 8: Recall=1.00 (5/5), ANN: 0.0155s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 3\n",
            "Query 9: Recall=1.00 (5/5), ANN: 0.0142s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 4\n",
            "Query 10: Recall=1.00 (5/5), ANN: 0.0156s, KNN: 0.0004s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 1.00\n",
            "Average times - ANN: 0.0228s, KNN: 0.0004s\n",
            "Average speedup: 0.02x\n",
            "â SUCCESS: Average recall (1.00) >= Target recall (0.70)\n",
            "\n",
            "--- Running ANN (with recall guarantees) using COSINE on GPU ---\n",
            "Dataset: 100 vectors, 2 dimensions, 10 queries, K=5\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 1: Recall=1.00 (5/5), ANN: 0.0180s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 2: Recall=1.00 (5/5), ANN: 0.0183s, KNN: 0.0006s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 3: Recall=1.00 (5/5), ANN: 0.0248s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 4: Recall=1.00 (5/5), ANN: 0.0172s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 5: Recall=1.00 (5/5), ANN: 0.0176s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 6: Recall=1.00 (5/5), ANN: 0.0169s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 7: Recall=1.00 (5/5), ANN: 0.0168s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 8: Recall=1.00 (5/5), ANN: 0.0167s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 9: Recall=1.00 (5/5), ANN: 0.0188s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 10: Recall=1.00 (5/5), ANN: 0.0178s, KNN: 0.0004s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 1.00\n",
            "Average times - ANN: 0.0183s, KNN: 0.0004s\n",
            "Average speedup: 0.02x\n",
            "â SUCCESS: Average recall (1.00) >= Target recall (0.70)\n",
            "\n",
            "--- ANN Test on LARGE dataset ---\n",
            "\n",
            "--- Running ANN (with recall guarantees) using L2 on GPU ---\n",
            "Dataset: 10000 vectors, 1024 dimensions, 100 queries, K=20\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 18\n",
            "Query 1: Recall=1.00 (20/20), ANN: 0.4431s, KNN: 0.0027s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 19\n",
            "Query 2: Recall=1.00 (20/20), ANN: 0.4571s, KNN: 0.0029s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 25\n",
            "Query 3: Recall=1.00 (20/20), ANN: 0.4675s, KNN: 0.0028s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 16\n",
            "Query 4: Recall=0.95 (19/20), ANN: 0.4224s, KNN: 0.0026s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 28\n",
            "Query 5: Recall=0.95 (19/20), ANN: 0.4545s, KNN: 0.0027s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 16\n",
            "Query 6: Recall=0.95 (19/20), ANN: 0.4471s, KNN: 0.0026s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 21\n",
            "Query 7: Recall=1.00 (20/20), ANN: 0.4358s, KNN: 0.0027s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 18\n",
            "Query 8: Recall=1.00 (20/20), ANN: 0.4907s, KNN: 0.0029s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 24\n",
            "Query 9: Recall=1.00 (20/20), ANN: 0.4498s, KNN: 0.0028s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 16\n",
            "Query 10: Recall=0.95 (19/20), ANN: 0.5333s, KNN: 0.0028s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 0.98\n",
            "Average times - ANN: 0.4601s, KNN: 0.0028s\n",
            "Average speedup: 0.01x\n",
            "â SUCCESS: Average recall (0.98) >= Target recall (0.70)\n",
            "\n",
            "--- Running ANN (with recall guarantees) using COSINE on GPU ---\n",
            "Dataset: 10000 vectors, 1024 dimensions, 100 queries, K=20\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 1: Recall=0.90 (18/20), ANN: 0.6322s, KNN: 0.0029s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 2: Recall=0.85 (17/20), ANN: 0.6138s, KNN: 0.0029s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 3: Recall=0.65 (13/20), ANN: 0.5427s, KNN: 0.0028s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 4: Recall=0.40 (8/20), ANN: 0.5244s, KNN: 0.0028s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 5: Recall=0.90 (18/20), ANN: 0.4909s, KNN: 0.0029s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 6: Recall=0.90 (18/20), ANN: 0.5105s, KNN: 0.0028s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 7: Recall=0.70 (14/20), ANN: 0.4918s, KNN: 0.0029s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 8: Recall=0.70 (14/20), ANN: 0.5111s, KNN: 0.0028s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 9: Recall=0.70 (14/20), ANN: 0.4922s, KNN: 0.0029s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 10: Recall=0.70 (14/20), ANN: 0.4925s, KNN: 0.0028s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 0.74\n",
            "Average times - ANN: 0.5302s, KNN: 0.0028s\n",
            "Average speedup: 0.01x\n",
            "â SUCCESS: Average recall (0.74) >= Target recall (0.70)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErEL7gqgZQsP",
        "outputId": "aee343da-e520-4693-ac13-9d3214414efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n",
            "/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "\n",
            "======= Testing on MEDIUM dataset =======\n",
            "\n",
            "--- Running KNN with L2 on CPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[2419, 1893, 1429,  718, 3803, 3614, 2734, 1142, 2692,   24],\n",
            "        [3698, 3513,  139, 1693, 2378, 2253,   67, 2342, 1369, 1229],\n",
            "        [1044, 3106,  979, 2440, 2540, 2859,  579, 3587,   74, 2721]])\n",
            "Time: 0.2562 sec\n",
            "\n",
            "--- Running KNN with COSINE on CPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[1142, 1893, 2680, 2091, 2419,   24,  317, 2692,  668, 3069],\n",
            "        [3698, 3513,  127, 1369, 2378, 3813, 2906, 2475,  660, 2253],\n",
            "        [1044, 3106, 2859, 3506,  734,  177,  579,  979, 2670, 2440]])\n",
            "Time: 0.0339 sec\n",
            "\n",
            "--- Running KNN with DOT on CPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[1558, 3019, 3197,  463, 2490, 1198,  180,  177, 3816, 3237],\n",
            "        [3582,  965, 1616,  192,  903, 2418, 2111, 3339,  448, 1104],\n",
            "        [ 193,  760, 3204, 1604, 1106, 3143, 2353, 1256,  804,  151]])\n",
            "Time: 0.0128 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on CPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[2419, 1142, 2734,   24, 3207, 3090, 3079, 1893, 1384, 3279],\n",
            "        [3698, 3513, 2906, 1693, 2342, 2475,  139, 1369, 2060, 1190],\n",
            "        [3106, 1044,  121,  979,  734, 2540, 2440, 3589, 3119, 2651]])\n",
            "Time: 0.0106 sec\n",
            "\n",
            "======= Testing on LARGE_DIM2 dataset =======\n",
            "\n",
            "--- Running KNN with L2 on CPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[8251,  734, 1618, 8168, 3693, 7539, 5941, 5343, 6855, 6376],\n",
            "        [6687, 7313, 4471, 3804, 2640, 4117, 3415, 8221, 9288, 8822],\n",
            "        [3553, 2499, 1982, 8125, 9428, 6636, 7578, 4357, 4031, 2602]])\n",
            "Time: 0.0015 sec\n",
            "\n",
            "--- Running KNN with COSINE on CPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[9555, 6279, 2439, 8251, 3985, 5258,  178, 5074, 6757, 3064],\n",
            "        [7832, 2418, 4781, 7816, 2993, 2350, 3421, 6560, 8822, 2226],\n",
            "        [ 967, 6337, 1549, 9428, 2108, 2914,  488, 9054, 4554, 3620]])\n",
            "Time: 0.0014 sec\n",
            "\n",
            "--- Running KNN with DOT on CPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[8191, 9633, 2973, 7712,  952, 6893, 7147, 8890, 8570, 2516],\n",
            "        [9814, 6893, 5128,  369, 3451, 9479, 8895,  803, 8109, 5457],\n",
            "        [4883, 1532, 5578, 2496, 3914, 7090, 1209, 4535, 5583, 9314]])\n",
            "Time: 0.0011 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on CPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[8251,  734, 1618, 8168, 3693, 7539, 5343, 6376,  391, 5941],\n",
            "        [6687, 4471, 7313, 4117, 3804, 2640, 9462, 3415, 8221, 9288],\n",
            "        [2499, 3553, 1982, 8125, 6636, 9428, 7578, 4031, 9788, 4357]])\n",
            "Time: 0.0180 sec\n",
            "\n",
            "======= Testing on LARGE_DIM32K dataset =======\n",
            "\n",
            "--- Running KNN with L2 on CPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[852, 703, 488, 925, 853, 522, 376, 153, 657,  65],\n",
            "        [339,  62, 959, 942, 872, 708, 408, 989, 753, 971],\n",
            "        [492, 738, 200, 901, 618, 708, 209,  54, 851, 255]])\n",
            "Time: 0.0332 sec\n",
            "\n",
            "--- Running KNN with COSINE on CPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[522, 703, 852, 258,  39, 448, 106, 268, 740, 925],\n",
            "        [308, 339,  34, 843, 908, 872, 942, 700,  20, 459],\n",
            "        [298, 618,  54, 209, 164, 662, 200,  21, 492, 308]])\n",
            "Time: 0.0393 sec\n",
            "\n",
            "--- Running KNN with DOT on CPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[538, 342, 635, 832,  97, 595, 870, 155, 392, 585],\n",
            "        [730, 912, 503, 735, 431, 557, 379, 723, 507, 364],\n",
            "        [197,  42, 234, 989, 743, 461,   7, 348, 896, 205]])\n",
            "Time: 0.0363 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on CPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[522, 376, 488, 852, 224, 419, 111, 703, 535, 867],\n",
            "        [ 62, 872, 959, 942, 708, 303, 848,  20, 339, 144],\n",
            "        [618, 200, 492, 708, 164, 209, 255,  54, 738, 770]])\n",
            "Time: 0.0395 sec\n",
            "\n",
            "======= [KMeans] Testing on SMALL dataset =======\n",
            "\n",
            "--- Running metric: L2 on CPU (100 samples) ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=L2\n",
            "K-Means converged at iteration 7\n",
            "First 10 labels: [3, 0, 0, 1, 2, 0, 3, 4, 2, 2]\n",
            "Time on CPU (L2): 0.0026 sec\n",
            "\n",
            "--- Running metric: cosine on CPU (100 samples) ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=cosine\n",
            "K-Means converged at iteration 6\n",
            "First 10 labels: [2, 0, 0, 2, 1, 0, 3, 4, 1, 4]\n",
            "Time on CPU (cosine): 0.0014 sec\n",
            "\n",
            "======= [KMeans] Testing on LARGE dataset =======\n",
            "\n",
            "--- Running metric: L2 on CPU (10000 samples) ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=L2\n",
            "K-Means converged at iteration 37\n",
            "First 10 labels: [11, 5, 0, 11, 10, 8, 1, 15, 5, 5]\n",
            "Time on CPU (L2): 2.7045 sec\n",
            "\n",
            "--- Running metric: cosine on CPU (10000 samples) ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=cosine\n",
            "K-Means converged at iteration 25\n",
            "First 10 labels: [8, 6, 7, 14, 19, 19, 16, 9, 16, 10]\n",
            "Time on CPU (cosine): 1.4233 sec\n",
            "\n",
            "======= Testing ANN with Mathematical Recall Guarantees =======\n",
            "\n",
            "--- ANN Test on SMALL dataset ---\n",
            "\n",
            "--- Running ANN (with recall guarantees) using L2 on CPU ---\n",
            "Dataset: 100 vectors, 2 dimensions, 10 queries, K=5\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 1: Recall=1.00 (5/5), ANN: 0.0112s, KNN: 0.0031s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 3\n",
            "Query 2: Recall=1.00 (5/5), ANN: 0.0093s, KNN: 0.0007s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 3: Recall=1.00 (5/5), ANN: 0.0090s, KNN: 0.0006s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 4: Recall=1.00 (5/5), ANN: 0.0097s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 5: Recall=1.00 (5/5), ANN: 0.0090s, KNN: 0.0006s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 12\n",
            "Query 6: Recall=1.00 (5/5), ANN: 0.0117s, KNN: 0.0006s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 7: Recall=1.00 (5/5), ANN: 0.0090s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 8: Recall=1.00 (5/5), ANN: 0.0076s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 9: Recall=1.00 (5/5), ANN: 0.0097s, KNN: 0.0006s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 10: Recall=1.00 (5/5), ANN: 0.0100s, KNN: 0.0005s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 1.00\n",
            "Average times - ANN: 0.0096s, KNN: 0.0008s\n",
            "Average speedup: 0.08x\n",
            "â SUCCESS: Average recall (1.00) >= Target recall (0.70)\n",
            "\n",
            "--- Running ANN (with recall guarantees) using COSINE on CPU ---\n",
            "Dataset: 100 vectors, 2 dimensions, 10 queries, K=5\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 1: Recall=1.00 (5/5), ANN: 0.0096s, KNN: 0.0006s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 4\n",
            "Query 2: Recall=1.00 (5/5), ANN: 0.0094s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 3: Recall=1.00 (5/5), ANN: 0.0106s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 4: Recall=1.00 (5/5), ANN: 0.0093s, KNN: 0.0006s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 4\n",
            "Query 5: Recall=1.00 (5/5), ANN: 0.0115s, KNN: 0.0007s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 3\n",
            "Query 6: Recall=1.00 (5/5), ANN: 0.0101s, KNN: 0.0008s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 7: Recall=1.00 (5/5), ANN: 0.0102s, KNN: 0.0007s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 8: Recall=1.00 (5/5), ANN: 0.0095s, KNN: 0.0007s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 3\n",
            "Query 9: Recall=1.00 (5/5), ANN: 0.0083s, KNN: 0.0007s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 10: Recall=1.00 (5/5), ANN: 0.0137s, KNN: 0.0006s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 1.00\n",
            "Average times - ANN: 0.0102s, KNN: 0.0006s\n",
            "Average speedup: 0.06x\n",
            "â SUCCESS: Average recall (1.00) >= Target recall (0.70)\n",
            "\n",
            "--- ANN Test on LARGE dataset ---\n",
            "\n",
            "--- Running ANN (with recall guarantees) using L2 on CPU ---\n",
            "Dataset: 10000 vectors, 1024 dimensions, 100 queries, K=20\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 23\n",
            "Query 1: Recall=1.00 (20/20), ANN: 9.0431s, KNN: 0.0120s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 19\n",
            "Query 2: Recall=0.95 (19/20), ANN: 9.1793s, KNN: 0.0104s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 22\n",
            "Query 3: Recall=1.00 (20/20), ANN: 9.4326s, KNN: 0.0105s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 16\n",
            "Query 4: Recall=0.95 (19/20), ANN: 10.3162s, KNN: 0.0112s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 19\n",
            "Query 5: Recall=1.00 (20/20), ANN: 9.0518s, KNN: 0.0109s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 14\n",
            "Query 6: Recall=0.95 (19/20), ANN: 8.8649s, KNN: 0.0104s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 21\n",
            "Query 7: Recall=1.00 (20/20), ANN: 9.6716s, KNN: 0.0108s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 19\n",
            "Query 8: Recall=0.95 (19/20), ANN: 8.9782s, KNN: 0.0114s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 20\n",
            "Query 9: Recall=1.00 (20/20), ANN: 8.9346s, KNN: 0.0105s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 21\n",
            "Query 10: Recall=1.00 (20/20), ANN: 9.5014s, KNN: 0.0106s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 0.98\n",
            "Average times - ANN: 9.2974s, KNN: 0.0109s\n",
            "Average speedup: 0.00x\n",
            "â SUCCESS: Average recall (0.98) >= Target recall (0.70)\n",
            "\n",
            "--- Running ANN (with recall guarantees) using COSINE on CPU ---\n",
            "Dataset: 10000 vectors, 1024 dimensions, 100 queries, K=20\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 13\n",
            "Query 1: Recall=0.65 (13/20), ANN: 5.1751s, KNN: 0.0134s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 10\n",
            "Query 2: Recall=0.90 (18/20), ANN: 5.3719s, KNN: 0.0105s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 10\n",
            "Query 3: Recall=0.75 (15/20), ANN: 4.8319s, KNN: 0.0130s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 4: Recall=0.60 (12/20), ANN: 5.4961s, KNN: 0.0111s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 10\n",
            "Query 5: Recall=0.90 (18/20), ANN: 4.9523s, KNN: 0.0102s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 11\n",
            "Query 6: Recall=0.65 (13/20), ANN: 5.2098s, KNN: 0.0116s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 10\n",
            "Query 7: Recall=0.70 (14/20), ANN: 5.1338s, KNN: 0.0107s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 8: Recall=0.65 (13/20), ANN: 4.8245s, KNN: 0.0103s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 9: Recall=0.75 (15/20), ANN: 5.6285s, KNN: 0.0113s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 12\n",
            "Query 10: Recall=0.75 (15/20), ANN: 5.1946s, KNN: 0.0111s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 0.73\n",
            "Average times - ANN: 5.1818s, KNN: 0.0113s\n",
            "Average speedup: 0.00x\n",
            "â SUCCESS: Average recall (0.73) >= Target recall (0.70)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py comparison"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm1Tm18Kd4cp",
        "outputId": "1769e413-690a-4708-ee9e-64ec9b3507c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======= [KNN] Comparing CPU vs GPU =======\n",
            "\n",
            "--- [KNN] Comparing on SMALL with L2 ---\n",
            "\n",
            "======= Comparing CPU vs GPU on SMALL dataset with L2 =======\n",
            "/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "CPU time: 0.2507 sec\n",
            "GPU time: 0.0007 sec\n",
            "GPU speedup: 369.83x\n",
            "\n",
            "--- [KNN] Comparing on SMALL with COSINE ---\n",
            "\n",
            "======= Comparing CPU vs GPU on SMALL dataset with cosine =======\n",
            "CPU time: 0.0308 sec\n",
            "GPU time: 0.0005 sec\n",
            "GPU speedup: 60.45x\n",
            "\n",
            "--- [KNN] Comparing on SMALL with DOT ---\n",
            "\n",
            "======= Comparing CPU vs GPU on SMALL dataset with dot =======\n",
            "CPU time: 0.0105 sec\n",
            "GPU time: 0.0003 sec\n",
            "GPU speedup: 35.98x\n",
            "\n",
            "--- [KNN] Comparing on SMALL with MANHATTAN ---\n",
            "\n",
            "======= Comparing CPU vs GPU on SMALL dataset with manhattan =======\n",
            "CPU time: 0.0018 sec\n",
            "GPU time: 0.0003 sec\n",
            "GPU speedup: 6.63x\n",
            "\n",
            "--- [KNN] Comparing on MEDIUM with L2 ---\n",
            "\n",
            "======= Comparing CPU vs GPU on MEDIUM dataset with L2 =======\n",
            "CPU time: 0.0080 sec\n",
            "GPU time: 0.0029 sec\n",
            "GPU speedup: 2.76x\n",
            "\n",
            "--- [KNN] Comparing on MEDIUM with COSINE ---\n",
            "\n",
            "======= Comparing CPU vs GPU on MEDIUM dataset with cosine =======\n",
            "CPU time: 0.0029 sec\n",
            "GPU time: 0.0058 sec\n",
            "GPU speedup: 0.50x\n",
            "\n",
            "--- [KNN] Comparing on MEDIUM with DOT ---\n",
            "\n",
            "======= Comparing CPU vs GPU on MEDIUM dataset with dot =======\n",
            "CPU time: 0.0023 sec\n",
            "GPU time: 0.0024 sec\n",
            "GPU speedup: 0.98x\n",
            "\n",
            "--- [KNN] Comparing on MEDIUM with MANHATTAN ---\n",
            "\n",
            "======= Comparing CPU vs GPU on MEDIUM dataset with manhattan =======\n",
            "CPU time: 0.0095 sec\n",
            "GPU time: 0.0093 sec\n",
            "GPU speedup: 1.02x\n",
            "\n",
            "--- [KNN] Comparing on LARGE with L2 ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE dataset with L2 =======\n",
            "CPU time: 0.0294 sec\n",
            "GPU time: 0.0290 sec\n",
            "GPU speedup: 1.01x\n",
            "\n",
            "--- [KNN] Comparing on LARGE with COSINE ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE dataset with cosine =======\n",
            "CPU time: 0.0278 sec\n",
            "GPU time: 0.0284 sec\n",
            "GPU speedup: 0.98x\n",
            "\n",
            "--- [KNN] Comparing on LARGE with DOT ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE dataset with dot =======\n",
            "CPU time: 0.0265 sec\n",
            "GPU time: 0.0236 sec\n",
            "GPU speedup: 1.12x\n",
            "\n",
            "--- [KNN] Comparing on LARGE with MANHATTAN ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE dataset with manhattan =======\n",
            "CPU time: 0.0533 sec\n",
            "GPU time: 0.0531 sec\n",
            "GPU speedup: 1.01x\n",
            "\n",
            "--- [KNN] Comparing on LARGE_DIM2 with L2 ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE_DIM2 dataset with L2 =======\n",
            "CPU time: 0.0021 sec\n",
            "GPU time: 0.0013 sec\n",
            "GPU speedup: 1.67x\n",
            "\n",
            "--- [KNN] Comparing on LARGE_DIM2 with COSINE ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE_DIM2 dataset with cosine =======\n",
            "CPU time: 0.0014 sec\n",
            "GPU time: 0.0013 sec\n",
            "GPU speedup: 1.11x\n",
            "\n",
            "--- [KNN] Comparing on LARGE_DIM2 with DOT ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE_DIM2 dataset with dot =======\n",
            "CPU time: 0.0010 sec\n",
            "GPU time: 0.0010 sec\n",
            "GPU speedup: 1.00x\n",
            "\n",
            "--- [KNN] Comparing on LARGE_DIM2 with MANHATTAN ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE_DIM2 dataset with manhattan =======\n",
            "CPU time: 0.0180 sec\n",
            "GPU time: 0.0180 sec\n",
            "GPU speedup: 1.00x\n",
            "\n",
            "--- [KNN] Comparing on LARGE_DIM32K with L2 ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE_DIM32K dataset with L2 =======\n",
            "CPU time: 0.0336 sec\n",
            "GPU time: 0.0393 sec\n",
            "GPU speedup: 0.86x\n",
            "\n",
            "--- [KNN] Comparing on LARGE_DIM32K with COSINE ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE_DIM32K dataset with cosine =======\n",
            "CPU time: 0.0341 sec\n",
            "GPU time: 0.0395 sec\n",
            "GPU speedup: 0.86x\n",
            "\n",
            "--- [KNN] Comparing on LARGE_DIM32K with DOT ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE_DIM32K dataset with dot =======\n",
            "CPU time: 0.0302 sec\n",
            "GPU time: 0.0335 sec\n",
            "GPU speedup: 0.90x\n",
            "\n",
            "--- [KNN] Comparing on LARGE_DIM32K with MANHATTAN ---\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE_DIM32K dataset with manhattan =======\n",
            "CPU time: 0.0353 sec\n",
            "GPU time: 0.0384 sec\n",
            "GPU speedup: 0.92x\n",
            "\n",
            "--- [KNN] Comparing on HUGE with L2 ---\n",
            "\n",
            "======= Comparing CPU vs GPU on HUGE dataset with L2 =======\n",
            "Note: Using 40,000 vectors to extrapolate performance for 4,000,000 vectors\n",
            "CPU time: 0.0166 sec\n",
            "GPU time: 0.0160 sec\n",
            "GPU speedup: 1.04x\n",
            "\n",
            "--- [KNN] Comparing on HUGE with COSINE ---\n",
            "\n",
            "======= Comparing CPU vs GPU on HUGE dataset with cosine =======\n",
            "Note: Using 40,000 vectors to extrapolate performance for 4,000,000 vectors\n",
            "CPU time: 0.0162 sec\n",
            "GPU time: 0.0154 sec\n",
            "GPU speedup: 1.05x\n",
            "\n",
            "--- [KNN] Comparing on HUGE with DOT ---\n",
            "\n",
            "======= Comparing CPU vs GPU on HUGE dataset with dot =======\n",
            "Note: Using 40,000 vectors to extrapolate performance for 4,000,000 vectors\n",
            "CPU time: 0.0138 sec\n",
            "GPU time: 0.0140 sec\n",
            "GPU speedup: 0.99x\n",
            "\n",
            "--- [KNN] Comparing on HUGE with MANHATTAN ---\n",
            "\n",
            "======= Comparing CPU vs GPU on HUGE dataset with manhattan =======\n",
            "Note: Using 40,000 vectors to extrapolate performance for 4,000,000 vectors\n",
            "CPU time: 0.0875 sec\n",
            "GPU time: 0.0775 sec\n",
            "GPU speedup: 1.13x\n",
            "\n",
            "======= [KMeans] Comparing CPU vs GPU =======\n",
            "\n",
            "--- [KMEANS] Running on SMALL with L2 ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=L2\n",
            "K-Means converged at iteration 6\n",
            "Running K-Means on PyTorch (CUDA backend), metric=L2\n",
            "K-Means converged at iteration 8\n",
            "CPU time: 0.0023 sec\n",
            "GPU time: 0.0701 sec\n",
            "GPU speedup: 0.03x\n",
            "\n",
            "--- [KMEANS] Running on SMALL with COSINE ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=cosine\n",
            "K-Means converged at iteration 7\n",
            "Running K-Means on PyTorch (CUDA backend), metric=cosine\n",
            "K-Means converged at iteration 7\n",
            "CPU time: 0.0018 sec\n",
            "GPU time: 0.0033 sec\n",
            "GPU speedup: 0.56x\n",
            "\n",
            "--- [KMEANS] Running on LARGE with L2 ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=L2\n",
            "K-Means converged at iteration 38\n",
            "Running K-Means on PyTorch (CUDA backend), metric=L2\n",
            "K-Means converged at iteration 50\n",
            "CPU time: 2.3691 sec\n",
            "GPU time: 0.1006 sec\n",
            "GPU speedup: 23.55x\n",
            "\n",
            "--- [KMEANS] Running on LARGE with COSINE ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=cosine\n",
            "K-Means converged at iteration 24\n",
            "Running K-Means on PyTorch (CUDA backend), metric=cosine\n",
            "K-Means converged at iteration 35\n",
            "CPU time: 0.9976 sec\n",
            "GPU time: 0.0575 sec\n",
            "GPU speedup: 17.36x\n",
            "\n",
            "======= [ANN] Comparing CPU vs GPU =======\n",
            "\n",
            "--- [ANN] Running on SMALL with L2 ---\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.9\n",
            "K-Means converged at iteration 6\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.9\n",
            "K-Means converged at iteration 5\n",
            "CPU time: 0.0066 sec\n",
            "GPU time: 0.0719 sec\n",
            "GPU speedup: 0.09x\n",
            "\n",
            "--- [ANN] Running on SMALL with COSINE ---\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.9\n",
            "K-Means converged at iteration 4\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.9\n",
            "K-Means converged at iteration 3\n",
            "CPU time: 0.0056 sec\n",
            "GPU time: 0.0157 sec\n",
            "GPU speedup: 0.36x\n",
            "\n",
            "--- [ANN] Running on LARGE with L2 ---\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=100, K2=100, desired_recall=0.9\n",
            "K-Means converged at iteration 21\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=100, K2=100, desired_recall=0.9\n",
            "K-Means converged at iteration 23\n",
            "CPU time: 9.2007 sec\n",
            "GPU time: 0.5644 sec\n",
            "GPU speedup: 16.30x\n",
            "\n",
            "--- [ANN] Running on LARGE with COSINE ---\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=100, K2=100, desired_recall=0.9\n",
            "K-Means converged at iteration 8\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=100, K2=100, desired_recall=0.9\n",
            "K-Means converged at iteration 10\n",
            "CPU time: 5.3151 sec\n",
            "GPU time: 0.7768 sec\n",
            "GPU speedup: 6.84x\n",
            "\n",
            "======= Extrapolating for HUGE dataset =======\n",
            "\n",
            "======= Extrapolating performance for 4,000,000 vectors =======\n",
            "Note: Using 40,000 vectors to extrapolate performance for 4,000,000 vectors\n",
            "Time for 40000 vectors: 0.0167 sec\n",
            "Estimated time for 4,000,000 vectors: 1.67 sec\n",
            "Optimizations needed for 4M vectors: batching, memory management, results streaming\n",
            "\n",
            "======= Extrapolating performance for 4,000,000 vectors =======\n",
            "Note: Using 40,000 vectors to extrapolate performance for 4,000,000 vectors\n",
            "Time for 40000 vectors: 0.0165 sec\n",
            "Estimated time for 4,000,000 vectors: 1.65 sec\n",
            "Optimizations needed for 4M vectors: batching, memory management, results streaming\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py all"
      ],
      "metadata": {
        "id": "XVn1nEITd_4W",
        "outputId": "bcc401d0-271c-4ef1-f965-f495a58ba023",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU: Tesla T4\n",
            "/usr/local/lib/python3.11/dist-packages/torch/__init__.py:1236: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at /pytorch/torch/csrc/tensor/python_tensor.cpp:434.)\n",
            "  _C._set_default_tensor_type(t)\n",
            "\n",
            "======= Testing on MEDIUM dataset =======\n",
            "\n",
            "--- Running KNN with L2 on GPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[2226, 1682,   91, 3448, 3058, 1175, 1690, 1583,  883,  535],\n",
            "        [3051,  413, 3761, 2909, 3406, 1224, 2843, 2698, 2386, 2938],\n",
            "        [2698, 3527, 1284, 1550, 2591, 1922,  985, 3651, 1060, 2741]],\n",
            "       device='cpu')\n",
            "Time: 0.1446 sec\n",
            "\n",
            "--- Running KNN with COSINE on GPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[2213, 2226, 3427, 2654, 2113, 1052, 1852, 1583,  468,    6],\n",
            "        [3835,  503,  413, 3051, 1285,  551, 3927, 1224, 3773,  458],\n",
            "        [1550, 1284, 1554, 2688, 1922, 2542, 2788, 3527,  911,  859]],\n",
            "       device='cpu')\n",
            "Time: 0.0347 sec\n",
            "\n",
            "--- Running KNN with DOT on GPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[ 705, 1197, 3509, 2405, 2630, 3674,  904, 2496, 1117, 3718],\n",
            "        [  42,  494,  537,  568, 1326, 2780, 2559, 1139, 2891, 2201],\n",
            "        [2091, 2732, 2647,  731,  836, 2851, 3574,   54, 1643, 1217]],\n",
            "       device='cpu')\n",
            "Time: 0.0125 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on GPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[2226, 1682, 3548, 3448, 2909, 1690, 2512,  535, 2113, 2040],\n",
            "        [3051, 3406,  302, 1224,  413, 2386, 1740, 3761, 3121, 2689],\n",
            "        [3527, 2698, 1284, 3651, 1550, 1060, 1922, 2591, 3115,   61]],\n",
            "       device='cpu')\n",
            "Time: 0.0105 sec\n",
            "\n",
            "======= Testing on LARGE_DIM2 dataset =======\n",
            "\n",
            "--- Running KNN with L2 on GPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[  59, 3190, 5103, 2557, 1696, 7050, 3263, 2224, 1514, 4931],\n",
            "        [4980, 8261, 1531,  870, 9031, 5794, 7540, 7863, 6509, 9755],\n",
            "        [  49, 7411, 9015, 6466, 8045, 8216, 9994, 3183, 6233, 4245]],\n",
            "       device='cpu')\n",
            "Time: 0.0017 sec\n",
            "\n",
            "--- Running KNN with COSINE on GPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[5136, 6058, 2573, 7269,  756, 2973, 4931, 8775, 1958, 4500],\n",
            "        [6509, 2212, 7717, 9956, 3757, 8411, 6382, 8261, 5813,  936],\n",
            "        [7492, 4835, 5891, 3917, 4763, 2341, 5341, 8263, 2154, 5349]],\n",
            "       device='cpu')\n",
            "Time: 0.0016 sec\n",
            "\n",
            "--- Running KNN with DOT on GPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[9115, 8821, 8070, 8623, 9894, 3257, 7097, 7729, 2458, 4768],\n",
            "        [ 615, 7890, 7338, 4104, 3308, 7145, 3776, 4755, 1338, 1487],\n",
            "        [1338, 7890,  615,  250, 7982, 4791, 8817, 7145, 5829, 3776]],\n",
            "       device='cpu')\n",
            "Time: 0.0012 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on GPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[  59, 3190, 1696, 2557, 5103, 3263, 7050, 2224, 7343, 4931],\n",
            "        [4980, 8261, 9031, 1531,  870, 5794, 6509, 7863, 3107, 7540],\n",
            "        [  49, 7411, 8216, 9015, 6233, 6466, 8045, 9994, 3183, 8178]],\n",
            "       device='cpu')\n",
            "Time: 0.0178 sec\n",
            "\n",
            "======= Testing on LARGE_DIM32K dataset =======\n",
            "\n",
            "--- Running KNN with L2 on GPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[535, 240, 318,  11, 332, 872, 416, 981,  73, 837],\n",
            "        [483, 689, 289, 240, 395, 667,  70,  88, 379, 580],\n",
            "        [483, 975, 529, 380, 951, 366, 924, 881, 415,  14]], device='cpu')\n",
            "Time: 0.0332 sec\n",
            "\n",
            "--- Running KNN with COSINE on GPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[332, 772, 535, 383, 318,  73, 466,  11, 277, 240],\n",
            "        [289, 198, 682, 689, 631, 112, 559, 831, 242,  70],\n",
            "        [366, 975, 331, 415, 268, 835, 874, 162, 380, 924]], device='cpu')\n",
            "Time: 0.0388 sec\n",
            "\n",
            "--- Running KNN with DOT on GPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[202, 557, 947, 619, 395, 699,  29, 117, 606, 330],\n",
            "        [459,   9, 813, 125, 509, 600, 705, 952, 981, 420],\n",
            "        [791, 643, 425, 240, 500, 349, 674, 686, 785, 192]], device='cpu')\n",
            "Time: 0.0360 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on GPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[ 11, 318, 240, 383, 535, 772, 332, 466,  73, 837],\n",
            "        [395, 580, 289, 283, 109, 303, 588, 716, 483, 689],\n",
            "        [483, 529, 975, 571, 336, 366, 380, 415, 260,  37]], device='cpu')\n",
            "Time: 0.0378 sec\n",
            "\n",
            "======= [KMeans] Testing on SMALL dataset =======\n",
            "\n",
            "--- Running metric: L2 on GPU (100 samples) ---\n",
            "Running K-Means on PyTorch (CUDA backend), metric=L2\n",
            "K-Means converged at iteration 4\n",
            "First 10 labels: [4, 3, 4, 4, 3, 1, 4, 2, 1, 2]\n",
            "Time on GPU (L2): 0.0728 sec\n",
            "\n",
            "--- Running metric: cosine on GPU (100 samples) ---\n",
            "Running K-Means on PyTorch (CUDA backend), metric=cosine\n",
            "K-Means converged at iteration 10\n",
            "First 10 labels: [1, 0, 2, 1, 4, 4, 1, 1, 2, 3]\n",
            "Time on GPU (cosine): 0.0043 sec\n",
            "\n",
            "======= [KMeans] Testing on LARGE dataset =======\n",
            "\n",
            "--- Running metric: L2 on GPU (10000 samples) ---\n",
            "Running K-Means on PyTorch (CUDA backend), metric=L2\n",
            "K-Means converged at iteration 38\n",
            "First 10 labels: [10, 5, 4, 1, 2, 5, 9, 9, 15, 0]\n",
            "Time on GPU (L2): 0.0765 sec\n",
            "\n",
            "--- Running metric: cosine on GPU (10000 samples) ---\n",
            "Running K-Means on PyTorch (CUDA backend), metric=cosine\n",
            "K-Means converged at iteration 22\n",
            "First 10 labels: [14, 18, 19, 8, 5, 19, 15, 6, 17, 12]\n",
            "Time on GPU (cosine): 0.0413 sec\n",
            "\n",
            "======= Testing ANN with Mathematical Recall Guarantees =======\n",
            "\n",
            "--- ANN Test on SMALL dataset ---\n",
            "\n",
            "--- Running ANN (with recall guarantees) using L2 on GPU ---\n",
            "Dataset: 100 vectors, 2 dimensions, 10 queries, K=5\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 10\n",
            "Query 1: Recall=1.00 (5/5), ANN: 0.0824s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 4\n",
            "Query 2: Recall=1.00 (5/5), ANN: 0.0149s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 3: Recall=1.00 (5/5), ANN: 0.0181s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 4: Recall=1.00 (5/5), ANN: 0.0165s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 5: Recall=1.00 (5/5), ANN: 0.0154s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 6: Recall=1.00 (5/5), ANN: 0.0162s, KNN: 0.0003s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 7: Recall=1.00 (5/5), ANN: 0.0158s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 14\n",
            "Query 8: Recall=1.00 (5/5), ANN: 0.0184s, KNN: 0.0003s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 9: Recall=1.00 (5/5), ANN: 0.0152s, KNN: 0.0003s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 10: Recall=1.00 (5/5), ANN: 0.0167s, KNN: 0.0003s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 1.00\n",
            "Average times - ANN: 0.0229s, KNN: 0.0004s\n",
            "Average speedup: 0.02x\n",
            "â SUCCESS: Average recall (1.00) >= Target recall (0.70)\n",
            "\n",
            "--- Running ANN (with recall guarantees) using COSINE on GPU ---\n",
            "Dataset: 100 vectors, 2 dimensions, 10 queries, K=5\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 1: Recall=1.00 (5/5), ANN: 0.0170s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 2: Recall=1.00 (5/5), ANN: 0.0181s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 3: Recall=1.00 (5/5), ANN: 0.0168s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 4: Recall=1.00 (5/5), ANN: 0.0172s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 5: Recall=1.00 (5/5), ANN: 0.0185s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 4\n",
            "Query 6: Recall=1.00 (5/5), ANN: 0.0164s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 7: Recall=1.00 (5/5), ANN: 0.0173s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 8: Recall=1.00 (5/5), ANN: 0.0180s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 3\n",
            "Query 9: Recall=1.00 (5/5), ANN: 0.0175s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 10: Recall=1.00 (5/5), ANN: 0.0180s, KNN: 0.0004s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 1.00\n",
            "Average times - ANN: 0.0175s, KNN: 0.0004s\n",
            "Average speedup: 0.02x\n",
            "â SUCCESS: Average recall (1.00) >= Target recall (0.70)\n",
            "\n",
            "--- ANN Test on LARGE dataset ---\n",
            "\n",
            "--- Running ANN (with recall guarantees) using L2 on GPU ---\n",
            "Dataset: 10000 vectors, 1024 dimensions, 100 queries, K=20\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 16\n",
            "Query 1: Recall=1.00 (20/20), ANN: 0.5165s, KNN: 0.0040s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 17\n",
            "Query 2: Recall=1.00 (20/20), ANN: 0.4966s, KNN: 0.0037s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 27\n",
            "Query 3: Recall=0.95 (19/20), ANN: 0.5939s, KNN: 0.0040s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 18\n",
            "Query 4: Recall=1.00 (20/20), ANN: 0.5976s, KNN: 0.0038s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 27\n",
            "Query 5: Recall=0.95 (19/20), ANN: 0.6461s, KNN: 0.0039s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 18\n",
            "Query 6: Recall=1.00 (20/20), ANN: 0.5853s, KNN: 0.0038s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 17\n",
            "Query 7: Recall=1.00 (20/20), ANN: 0.4652s, KNN: 0.0037s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 27\n",
            "Query 8: Recall=1.00 (20/20), ANN: 0.5213s, KNN: 0.0037s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 28\n",
            "Query 9: Recall=0.95 (19/20), ANN: 0.5475s, KNN: 0.0043s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 19\n",
            "Query 10: Recall=1.00 (20/20), ANN: 0.5029s, KNN: 0.0038s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 0.98\n",
            "Average times - ANN: 0.5473s, KNN: 0.0039s\n",
            "Average speedup: 0.01x\n",
            "â SUCCESS: Average recall (0.98) >= Target recall (0.70)\n",
            "\n",
            "--- Running ANN (with recall guarantees) using COSINE on GPU ---\n",
            "Dataset: 10000 vectors, 1024 dimensions, 100 queries, K=20\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 12\n",
            "Query 1: Recall=0.75 (15/20), ANN: 0.5327s, KNN: 0.0038s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 11\n",
            "Query 2: Recall=0.70 (14/20), ANN: 0.5859s, KNN: 0.0038s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 3: Recall=0.80 (16/20), ANN: 0.5335s, KNN: 0.0038s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 4: Recall=0.75 (15/20), ANN: 0.5632s, KNN: 0.0041s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 12\n",
            "Query 5: Recall=0.80 (16/20), ANN: 0.5425s, KNN: 0.0043s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 6: Recall=0.70 (14/20), ANN: 0.5503s, KNN: 0.0038s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 7: Recall=0.70 (14/20), ANN: 0.5270s, KNN: 0.0038s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 11\n",
            "Query 8: Recall=0.75 (15/20), ANN: 0.5260s, KNN: 0.0038s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 9: Recall=0.75 (15/20), ANN: 0.5531s, KNN: 0.0038s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 10: Recall=0.95 (19/20), ANN: 0.5221s, KNN: 0.0038s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 0.77\n",
            "Average times - ANN: 0.5436s, KNN: 0.0039s\n",
            "Average speedup: 0.01x\n",
            "â SUCCESS: Average recall (0.77) >= Target recall (0.70)\n",
            "Using CPU\n",
            "\n",
            "======= Testing on MEDIUM dataset =======\n",
            "\n",
            "--- Running KNN with L2 on CPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[1600,  707, 3946, 2770, 1920,  671, 2726, 3008,  224,  325],\n",
            "        [3808, 2247,  759,   14, 3141,  986, 3204, 2294,  215, 1050],\n",
            "        [3399, 2434, 3780,  239, 3008, 1600, 3217, 1372, 2711, 2290]])\n",
            "Time: 0.0042 sec\n",
            "\n",
            "--- Running KNN with COSINE on CPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[ 871, 3946,   79,  325, 1600, 1358, 2010, 2799, 3794, 2239],\n",
            "        [  14, 2133,  215, 2247,  921, 2638, 3450, 3627, 1050, 3436],\n",
            "        [1372,  467, 3399,  406,  239, 2434,  238,  624, 2290,   93]])\n",
            "Time: 0.0024 sec\n",
            "\n",
            "--- Running KNN with DOT on CPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[1489, 3001, 1322, 3988,   17,  944,  625, 3790,  530, 3200],\n",
            "        [3101,  259, 1213, 1252, 3958,  110, 1335,  625, 2678,  906],\n",
            "        [2882, 1850, 3078, 3987, 3576, 2552,  920, 3159, 3365, 2207]])\n",
            "Time: 0.0017 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on CPU (4000 db, 100 queries, 128 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[1600, 3946, 1920, 1696,   79, 2817,  707, 1125, 1400, 1625],\n",
            "        [ 759, 2247, 3627,   14,  215, 1715, 1626, 2405,  986, 1338],\n",
            "        [3399,  239, 3780, 1372, 2434, 1644, 2266, 2849, 2573,  406]])\n",
            "Time: 0.0044 sec\n",
            "\n",
            "======= Testing on LARGE_DIM2 dataset =======\n",
            "\n",
            "--- Running KNN with L2 on CPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[7253, 1641, 3132, 8564, 7935, 5910, 6317, 9793, 9497,  663],\n",
            "        [5241, 6429, 8514, 3300, 7033, 9125, 1331, 9678, 5056, 1119],\n",
            "        [2991, 9936, 8046, 3403, 9905, 6904, 2955, 6399, 6309, 2752]])\n",
            "Time: 0.0010 sec\n",
            "\n",
            "--- Running KNN with COSINE on CPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[6497, 1897, 8636, 3983, 6857, 9360,  347, 9730, 9599, 2633],\n",
            "        [7879, 5241, 3965, 2456, 2560, 8230,  337, 6685, 3300,  213],\n",
            "        [ 304, 8572, 2024, 7999, 2752, 5982, 6309, 5133, 3165, 1662]])\n",
            "Time: 0.0010 sec\n",
            "\n",
            "--- Running KNN with DOT on CPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[7220, 1476, 6228, 3244, 4608, 2568,   65, 6220, 9093, 5260],\n",
            "        [ 796, 1839, 2374, 4505, 9753, 4006,   28, 1881, 4247, 7147],\n",
            "        [7421, 2189, 9152, 5427, 2527, 9038, 5815, 5521,  693, 1726]])\n",
            "Time: 0.0007 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on CPU (10000 db, 100 queries, 2 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[7253, 1641, 3132, 8564, 7935, 5910, 6317, 9793, 9497,  663],\n",
            "        [5241, 6429, 8514, 3300, 7033, 9125, 9678, 5056, 1331, 1119],\n",
            "        [2991, 9936, 8046, 3403, 6399, 6904, 9905, 2955, 2752, 6309]])\n",
            "Time: 0.0072 sec\n",
            "\n",
            "======= Testing on LARGE_DIM32K dataset =======\n",
            "\n",
            "--- Running KNN with L2 on CPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[223, 220, 391, 594, 528, 612, 840, 979, 523, 868],\n",
            "        [698, 776, 932, 224, 463, 523, 231, 900,  99, 534],\n",
            "        [288, 549, 868, 694, 112, 421, 301, 176, 514,  17]])\n",
            "Time: 0.0344 sec\n",
            "\n",
            "--- Running KNN with COSINE on CPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[979, 391, 768, 220, 387, 766, 333,  11, 661, 870],\n",
            "        [698,  99, 776, 932, 523, 900, 391, 819, 927, 706],\n",
            "        [922, 176, 549, 288, 480,  91, 465, 301, 803, 964]])\n",
            "Time: 0.0354 sec\n",
            "\n",
            "--- Running KNN with DOT on CPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[131,  78, 939, 439, 598, 390, 141, 648, 852, 115],\n",
            "        [775,  63, 427,  67,  56, 578, 615, 301, 547, 823],\n",
            "        [999, 102, 395, 189, 779, 140, 967, 177, 845, 295]])\n",
            "Time: 0.0314 sec\n",
            "\n",
            "--- Running KNN with MANHATTAN on CPU (1000 db, 10 queries, 32768 dimensions) ---\n",
            "First 3 query results:\n",
            "tensor([[594, 481, 596, 523, 223, 391, 528, 831,  39, 793],\n",
            "        [698, 932, 224, 770, 378, 534, 776, 654,  99, 231],\n",
            "        [710, 549, 421,  17, 225, 392, 288, 868, 694, 480]])\n",
            "Time: 0.0355 sec\n",
            "\n",
            "======= [KMeans] Testing on SMALL dataset =======\n",
            "\n",
            "--- Running metric: L2 on CPU (100 samples) ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=L2\n",
            "K-Means converged at iteration 8\n",
            "First 10 labels: [4, 3, 4, 4, 3, 0, 4, 1, 4, 3]\n",
            "Time on CPU (L2): 0.0030 sec\n",
            "\n",
            "--- Running metric: cosine on CPU (100 samples) ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=cosine\n",
            "K-Means converged at iteration 5\n",
            "First 10 labels: [4, 3, 4, 4, 3, 2, 4, 4, 4, 3]\n",
            "Time on CPU (cosine): 0.0018 sec\n",
            "\n",
            "======= [KMeans] Testing on LARGE dataset =======\n",
            "\n",
            "--- Running metric: L2 on CPU (10000 samples) ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=L2\n",
            "K-Means converged at iteration 30\n",
            "First 10 labels: [18, 6, 11, 8, 11, 4, 19, 11, 1, 11]\n",
            "Time on CPU (L2): 2.5202 sec\n",
            "\n",
            "--- Running metric: cosine on CPU (10000 samples) ---\n",
            "Running K-Means on PyTorch (CPU backend), metric=cosine\n",
            "K-Means converged at iteration 38\n",
            "First 10 labels: [1, 19, 0, 16, 9, 17, 15, 10, 17, 2]\n",
            "Time on CPU (cosine): 1.8329 sec\n",
            "\n",
            "======= Testing ANN with Mathematical Recall Guarantees =======\n",
            "\n",
            "--- ANN Test on SMALL dataset ---\n",
            "\n",
            "--- Running ANN (with recall guarantees) using L2 on CPU ---\n",
            "Dataset: 100 vectors, 2 dimensions, 10 queries, K=5\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 1: Recall=1.00 (5/5), ANN: 0.0070s, KNN: 0.0006s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 4\n",
            "Query 2: Recall=1.00 (5/5), ANN: 0.0069s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 3: Recall=1.00 (5/5), ANN: 0.0061s, KNN: 0.0006s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 4: Recall=1.00 (5/5), ANN: 0.0102s, KNN: 0.0007s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 5: Recall=1.00 (5/5), ANN: 0.0107s, KNN: 0.0006s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 6: Recall=1.00 (5/5), ANN: 0.0101s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 7: Recall=1.00 (5/5), ANN: 0.0062s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 8: Recall=1.00 (5/5), ANN: 0.0056s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 4\n",
            "Query 9: Recall=1.00 (5/5), ANN: 0.0056s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 10: Recall=1.00 (5/5), ANN: 0.0058s, KNN: 0.0005s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 1.00\n",
            "Average times - ANN: 0.0074s, KNN: 0.0005s\n",
            "Average speedup: 0.07x\n",
            "â SUCCESS: Average recall (1.00) >= Target recall (0.70)\n",
            "\n",
            "--- Running ANN (with recall guarantees) using COSINE on CPU ---\n",
            "Dataset: 100 vectors, 2 dimensions, 10 queries, K=5\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 4\n",
            "Query 1: Recall=1.00 (5/5), ANN: 0.0065s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 4\n",
            "Query 2: Recall=1.00 (5/5), ANN: 0.0065s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 3: Recall=1.00 (5/5), ANN: 0.0079s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 6\n",
            "Query 4: Recall=1.00 (5/5), ANN: 0.0066s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 8\n",
            "Query 5: Recall=1.00 (5/5), ANN: 0.0070s, KNN: 0.0004s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 3\n",
            "Query 6: Recall=1.00 (5/5), ANN: 0.0064s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 3\n",
            "Query 7: Recall=1.00 (5/5), ANN: 0.0062s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 8: Recall=1.00 (5/5), ANN: 0.0075s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 5\n",
            "Query 9: Recall=1.00 (5/5), ANN: 0.0076s, KNN: 0.0005s\n",
            "ANN parameters (recall guarantee): num_clusters=10, K1=10, K2=25, desired_recall=0.7\n",
            "K-Means converged at iteration 3\n",
            "Query 10: Recall=1.00 (5/5), ANN: 0.0071s, KNN: 0.0005s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 1.00\n",
            "Average times - ANN: 0.0069s, KNN: 0.0005s\n",
            "Average speedup: 0.07x\n",
            "â SUCCESS: Average recall (1.00) >= Target recall (0.70)\n",
            "\n",
            "--- ANN Test on LARGE dataset ---\n",
            "\n",
            "--- Running ANN (with recall guarantees) using L2 on CPU ---\n",
            "Dataset: 10000 vectors, 1024 dimensions, 100 queries, K=20\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 26\n",
            "Query 1: Recall=1.00 (20/20), ANN: 9.9104s, KNN: 0.0111s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 20\n",
            "Query 2: Recall=1.00 (20/20), ANN: 9.2710s, KNN: 0.0106s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 18\n",
            "Query 3: Recall=1.00 (20/20), ANN: 9.5641s, KNN: 0.0103s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 22\n",
            "Query 4: Recall=0.95 (19/20), ANN: 9.9283s, KNN: 0.0109s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 22\n",
            "Query 5: Recall=1.00 (20/20), ANN: 9.6189s, KNN: 0.0116s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 18\n",
            "Query 6: Recall=0.95 (19/20), ANN: 9.0143s, KNN: 0.0106s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 23\n",
            "Query 7: Recall=1.00 (20/20), ANN: 9.9552s, KNN: 0.0112s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 15\n",
            "Query 8: Recall=1.00 (20/20), ANN: 8.9611s, KNN: 0.0105s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 26\n",
            "Query 9: Recall=1.00 (20/20), ANN: 9.8219s, KNN: 0.0117s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 20\n",
            "Query 10: Recall=0.95 (19/20), ANN: 9.0398s, KNN: 0.0119s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 0.98\n",
            "Average times - ANN: 9.5085s, KNN: 0.0110s\n",
            "Average speedup: 0.00x\n",
            "â SUCCESS: Average recall (0.98) >= Target recall (0.70)\n",
            "\n",
            "--- Running ANN (with recall guarantees) using COSINE on CPU ---\n",
            "Dataset: 10000 vectors, 1024 dimensions, 100 queries, K=20\n",
            "\n",
            "=== Testing with target recall: 0.70 ===\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 1: Recall=0.65 (13/20), ANN: 5.5055s, KNN: 0.0102s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 2: Recall=0.60 (12/20), ANN: 4.6909s, KNN: 0.0103s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 12\n",
            "Query 3: Recall=0.75 (15/20), ANN: 5.4957s, KNN: 0.0118s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 10\n",
            "Query 4: Recall=0.70 (14/20), ANN: 5.1213s, KNN: 0.0116s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 17\n",
            "Query 5: Recall=0.80 (16/20), ANN: 5.2861s, KNN: 0.0104s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 12\n",
            "Query 6: Recall=0.75 (15/20), ANN: 5.7158s, KNN: 0.0102s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 7: Recall=0.75 (15/20), ANN: 4.6339s, KNN: 0.0106s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 9\n",
            "Query 8: Recall=0.80 (16/20), ANN: 5.5062s, KNN: 0.0102s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 11\n",
            "Query 9: Recall=0.50 (10/20), ANN: 4.9484s, KNN: 0.0103s\n",
            "ANN parameters (recall guarantee): num_clusters=100, K1=64, K2=100, desired_recall=0.7\n",
            "K-Means converged at iteration 7\n",
            "Query 10: Recall=0.75 (15/20), ANN: 4.9432s, KNN: 0.0110s\n",
            "\n",
            "Target recall: 0.70\n",
            "Actual average recall: 0.70\n",
            "Average times - ANN: 5.1847s, KNN: 0.0106s\n",
            "Average speedup: 0.00x\n",
            "â SUCCESS: Average recall (0.70) >= Target recall (0.70)\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE_DIM2 dataset with L2 =======\n",
            "CPU time: 0.0029 sec\n",
            "GPU time: 0.0015 sec\n",
            "GPU speedup: 1.97x\n",
            "\n",
            "======= Comparing CPU vs GPU on LARGE_DIM32K dataset with L2 =======\n",
            "CPU time: 0.0354 sec\n",
            "GPU time: 0.0405 sec\n",
            "GPU speedup: 0.87x\n",
            "\n",
            "======= Extrapolating performance for 4,000,000 vectors =======\n",
            "Note: Using 40,000 vectors to extrapolate performance for 4,000,000 vectors\n",
            "Time for 40000 vectors: 0.0156 sec\n",
            "Estimated time for 4,000,000 vectors: 1.56 sec\n",
            "Optimizations needed for 4M vectors: batching, memory management, results streaming\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-GzKkzCSZTm-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}